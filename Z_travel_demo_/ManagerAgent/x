This is great. Having your actual code allows us to perform a "surgical" migration. Your current setup is a classic Stateful Multi-Agent System that relies on a centralized MySQL database for its business logic (the "process maps") and in-memory persistence for short-term chat memory.
To move this to Google Cloud (Vertex AI/Cloud Run), we need to address three specific code-level shifts:
 * Database Strategy: Swapping mysql-connector for the cloud-sql-python-connector to handle GCP's networking.
 * Persistence Strategy: Moving from MemorySaver() (which wipes every time the Cloud Run instance restarts) to a Cloud SQL-backed Checkpointer for production-grade persistence.
 * Environment Parity: Moving hardcoded configurations (like PHOENIX_COLLECTOR_ENDPOINT) into environment variables.
1. Code Level: db_connection.py
We need to modify your DBConnection class to support the GCP Cloud SQL Connector. This avoids the need for IP Whitelisting or complex firewall rules.
import os
import pymysql # Use pymysql with the Cloud SQL Connector
from google.cloud.sql.connector import Connector, IPTypes
from config import configuration

class DBConnection:
    def __init__(self):
        # We now use environment variables set in the GCP Console
        self.db_type = os.getenv("DB_TYPE", "db_dev")
        self.instance_connection_name = os.getenv("INSTANCE_CONNECTION_NAME") # "project:region:instance"
        self.db_user = os.getenv("DB_USER")
        self.db_pass = os.getenv("DB_PASS")
        self.db_name = os.getenv("DB_NAME")
        
        # Local fallback logic if you are still testing on VM
        self.is_local = os.getenv("IS_LOCAL", "true").lower() == "true"
        
        self.db_enabled = True # Force enabled for cloud
        self.connector = Connector()

    def _connect(self):
        # If running in GCP, use the secure connector
        if not self.is_local:
            return self.connector.connect(
                self.instance_connection_name,
                "pymysql",
                user=self.db_user,
                password=self.db_pass,
                db=self.db_name,
                ip_type=IPTypes.PUBLIC
            )
        else:
            # Your existing local connection logic
            import mysql.connector
            config = configuration[self.db_type]
            return mysql.connector.connect(
                host=config['host'], port=config['port'],
                user=config['user'], password=config['password'],
                database=config['database']
            )

    # Note: Keep your insert(), fetch_one(), etc. identical. 
    # Just ensure you handle the cursor type correctly (pymysql vs mysql.connector).
    def fetch_one(self, query, params=()):
        conn = self._connect()
        # Handle difference in cursor initialization
        cursor = conn.cursor(pymysql.cursors.DictCursor) if not self.is_local else conn.cursor(dictionary=True)
        query = query.replace("?", "%s")
        cursor.execute(query, params)
        result = cursor.fetchone()
        conn.close()
        return result

2. Code Level: graph.py
Currently, you use MemorySaver(). In Cloud Run, instances "scale to zero" or restart. MemorySaver will lose all your chat progress.
Recommendation: Swap MemorySaver for a MySQL-based checkpointer (using your Cloud SQL instance).
# Updated get_graph function in graph.py
from langgraph.checkpoint.mysql import MySQLSaver # You'll need to install langgraph-checkpoint-mysql

def get_graph(with_memory: bool = True):
    graph = build_graph()
    
    if with_memory:
        # Check if we are in production/cloud
        if os.getenv("IS_LOCAL") == "false":
            # Use Cloud SQL for persistence
            db_conn = DBConnection()
            checkpointer = MySQLSaver(db_conn._connect()) 
            return graph.compile(checkpointer=checkpointer)
        else:
            return graph.compile(checkpointer=MemorySaver())
    return graph.compile()

3. Code Level: main.py
We need to make the API "Cloud-Native" by removing hardcoded host/port and cleaning up the Arize Phoenix setup for the cloud.
# main.py changes
import os

# 1. Dynamic Port for Cloud Run
PORT = int(os.environ.get("PORT", 7006))

# 2. Conditional Observability
if os.getenv("ENABLE_TRACING") == "true":
    from phoenix.otel import register
    register(
        project_name=os.getenv("PHOENIX_PROJECT", "healthcare-outreach"),
        endpoint=os.getenv("PHOENIX_COLLECTOR_ENDPOINT")
    )

# 3. Use the correct URL from your config
demo_endpoint = configuration['api_url']['demo_url']

@app.post(f"{demo_endpoint}/generatemockcall")
async def mock_call(query: MockQuery):
    # Your existing logic remains 95% the same!
    # The 'sessions' dict should ideally be moved to Redis or Cloud SQL 
    # if you want to share session metadata across multiple Cloud Run instances.
    ...

if __name__ == "__main__":
    # Change host to 0.0.0.0 for GCP visibility
    uvicorn.run(app, host="0.0.0.0", port=PORT)

4. The requirements.txt (The Migration "Engine")
Since you cannot use Docker locally, this file is what Google Cloud Build will use to build your environment. Add these to your existing file:
fastapi
uvicorn
langgraph
langgraph-checkpoint-mysql
langchain-openai
google-cloud-sql-connector[pymysql]
pymysql
cryptography
pydantic
arize-phoenix

Summary of what you need to do in the GCP Console:
 * Cloud SQL: Create a MySQL instance.
 * Secret Manager: (Optional) Put your OPENAI_API_KEY and DB_PASS here.
 * Deployment: Run the gcloud run deploy command I provided in the previous turn.
Next Step
Would you like me to help you write a Python script to initialize the MySQL tables specifically for the MySQLSaver checkpointer so your graph's persistence is ready before you deploy?
