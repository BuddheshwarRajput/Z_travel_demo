import os
import pandas as pd
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel
from config import configuration
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from pydantic import BaseModel, Field

# --- IMPORT 1: Get the shared Langfuse handler
from components.llm import langfuse_handler

# --- IMPORT 2: Get the Console Handler (Restores your terminal logs)
from langchain_core.callbacks import StdOutCallbackHandler

# Set environment variables
os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

llm = AzureChatOpenAI(**{'deployment_name': configuration['open_ai_config']['deployment_name'], 
                         'openai_api_version': configuration['open_ai_config']['openai_api_version']}, 
                      temperature=0.1)

# Initialize Console Handler
std_out_handler = StdOutCallbackHandler()

class recommend_output_format(BaseModel):
    Recommendation: list[str] = Field(description="returns a list of recommendations")

class Evaluator:
    def __init__(self):
        self.history = ""

    def calculate_timestamp_difference_tool(self, messages):
        df = pd.DataFrame(messages)
        if df.empty or 'timestamp' not in df.columns:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time...','description': 'Data unavailable','flag': 1}]}]

        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S:%f')
        df['timestamp_difference'] = df['timestamp'].diff().dt.total_seconds() * 1000
        df['more_than_20s'] = df['timestamp_difference'] > 20000
        
        agent_msgs = df[df['messenger']=="Agent"]
        if not agent_msgs.empty and True in set(agent_msgs['more_than_20s']):
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did exceed silent time of 20 seconds...','flag': 0}]}]
        else:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did not exceed silent time of 20 seconds...','flag': 1}]}]

    def load_parameters(self, sub_param_df):
        param_descriptions = {}
        for _, row in sub_param_df.iterrows():
            parameter = row['parameter_name']
            description = row['std_parameter_desc']
            if parameter not in param_descriptions:
                param_descriptions[parameter] = []
            param_descriptions[parameter].append(description)
        return param_descriptions

    def recommendation_generator(self, input_json, run_config=None):
        recommendation_template="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation.
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        
        input_json:
        {input_json}
        
        format_instructions:
        {format_instructions}
        """
        recommendation_template_without_format="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation.
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        4.Only give recommendations do not output the parameter name as title or heading.
        
        input_json:
        {input_json}
        """
        
        parser = JsonOutputParser(pydantic_object=recommend_output_format)
        
        prompt_rec = PromptTemplate(
            input_variables=["input_json"],
            partial_variables={"format_instructions": parser.get_format_instructions()},
            template=recommendation_template)
            
        prompt_rec_no_fmt = PromptTemplate(
            input_variables=["input_json"],
            template=recommendation_template_without_format)
        
        llm_chain_rec = prompt_rec | llm | parser
        llm_chain_rec_no_fmt = prompt_rec_no_fmt | llm | StrOutputParser()
        
        config = run_config if run_config else {}
        
        gen_recommendations = llm_chain_rec.invoke({"input_json":input_json}, config=config)
        
        # Original print statement (now visible again)
        print("gen_recommendations---------", gen_recommendations)
        
        if "Recommendation" in gen_recommendations.keys():
            print("recommendation found")
            return gen_recommendations['Recommendation']
        else:
            gen_recommendations = llm_chain_rec_no_fmt.invoke({"input_json":input_json}, config=config)
            try:
                gen_recommendations=[line.split(". ",1)[1] for line in gen_recommendations.splitlines() if line.strip()]
            except:
                gen_recommendations=[line for line in gen_recommendations.splitlines() if line.strip()]
            print("recommendation not found")
            return gen_recommendations

    def report_generator(self, chains, conversation_history, conversation_history_list, param_descriptions, session_id=None, demo_name=None, domain=None):        
        """
        Generate report. Uses BOTH Langfuse (for web) and StdOut (for terminal).
        """
        
        # <--- KEY UPDATE: Combine Both Handlers
        # This restores your terminal logs AND sends data to Langfuse
        combined_callbacks = [langfuse_handler, std_out_handler]

        run_config = {
            "callbacks": combined_callbacks,
            "metadata": {
                "session_id": session_id,
                "demo_name": demo_name,
                "domain": domain,
                "type": "evaluation"
            },
            # Using 'tags' in config metadata is safer than constructor
            "tags": [demo_name, domain, "evaluation"]
        }
        
        tool_output = self.calculate_timestamp_difference_tool(conversation_history_list)
        
        map_chain = RunnableParallel(**chains)
        
        # Pass the Combined Config
        response = map_chain.invoke(
            {"conversation_history": conversation_history, "tool_output": tool_output},
            config=run_config
        )
        
        # Pass the Combined Config
        response['Recommendation'] = self.recommendation_generator(response, run_config=run_config)
        
        if hasattr(langfuse_handler, 'flush'):
            langfuse_handler.flush()
        
        return response









import os
import pandas as pd
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel
from config import configuration
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from pydantic import BaseModel, Field

# <--- KEY FIX: Import the working handler from your LLM file
from components.llm import langfuse_handler

# Set environment variables
os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

llm = AzureChatOpenAI(**{'deployment_name': configuration['open_ai_config']['deployment_name'], 
                         'openai_api_version': configuration['open_ai_config']['openai_api_version']}, 
                      temperature=0.1)

class recommend_output_format(BaseModel):
    Recommendation: list[str] = Field(description="returns a list of recommendations")

class Evaluator:
    def __init__(self):
        self.history = ""

    def calculate_timestamp_difference_tool(self, messages):
        df = pd.DataFrame(messages)
        if df.empty or 'timestamp' not in df.columns:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time...','description': 'Data unavailable','flag': 1}]}]

        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S:%f')
        df['timestamp_difference'] = df['timestamp'].diff().dt.total_seconds() * 1000
        df['more_than_20s'] = df['timestamp_difference'] > 20000
        
        agent_msgs = df[df['messenger']=="Agent"]
        if not agent_msgs.empty and True in set(agent_msgs['more_than_20s']):
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did exceed silent time of 20 seconds...','flag': 0}]}]
        else:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did not exceed silent time of 20 seconds...','flag': 1}]}]

    def load_parameters(self, sub_param_df):
        param_descriptions = {}
        for _, row in sub_param_df.iterrows():
            parameter = row['parameter_name']
            description = row['std_parameter_desc']
            if parameter not in param_descriptions:
                param_descriptions[parameter] = []
            param_descriptions[parameter].append(description)
        return param_descriptions

    # <--- MODIFIED: Accepts config dictionary directly
    def recommendation_generator(self, input_json, run_config=None):
        recommendation_template="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation.
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        
        input_json:
        {input_json}
        
        format_instructions:
        {format_instructions}
        """
        recommendation_template_without_format="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation.
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        4.Only give recommendations do not output the parameter name as title or heading.
        
        input_json:
        {input_json}
        """
        
        parser = JsonOutputParser(pydantic_object=recommend_output_format)
        
        prompt_rec = PromptTemplate(
            input_variables=["input_json"],
            partial_variables={"format_instructions": parser.get_format_instructions()},
            template=recommendation_template)
            
        prompt_rec_no_fmt = PromptTemplate(
            input_variables=["input_json"],
            template=recommendation_template_without_format)
        
        llm_chain_rec = prompt_rec | llm | parser
        llm_chain_rec_no_fmt = prompt_rec_no_fmt | llm | StrOutputParser()
        
        # Use the config passed from report_generator (contains handler + metadata)
        config = run_config if run_config else {}
        
        gen_recommendations = llm_chain_rec.invoke({"input_json":input_json}, config=config)
        
        if "Recommendation" in gen_recommendations.keys():
            return gen_recommendations['Recommendation']
        else:
            gen_recommendations = llm_chain_rec_no_fmt.invoke({"input_json":input_json}, config=config)
            try:
                gen_recommendations=[line.split(". ",1)[1] for line in gen_recommendations.splitlines() if line.strip()]
            except:
                gen_recommendations=[line for line in gen_recommendations.splitlines() if line.strip()]
            return gen_recommendations

    def report_generator(self, chains, conversation_history, conversation_history_list, param_descriptions, session_id=None, demo_name=None, domain=None):        
        """
        Generate report. Uses the global langfuse_handler from components.llm
        """
        
        # <--- KEY FIX: Prepare the config dictionary
        # We pass the handler here, AND we pass the session_id as metadata
        # This prevents the constructor crash.
        run_config = {
            "callbacks": [langfuse_handler],
            "metadata": {
                "session_id": session_id,
                "demo_name": demo_name,
                "domain": domain,
                "type": "evaluation"
            },
            "tags": [demo_name, domain, "evaluation"]
        }
        
        tool_output = self.calculate_timestamp_difference_tool(conversation_history_list)
        
        map_chain = RunnableParallel(**chains)
        
        # Pass the Safe Config to the chain
        response = map_chain.invoke(
            {"conversation_history": conversation_history, "tool_output": tool_output},
            config=run_config
        )
        
        # Pass the Safe Config to the recommendation generator
        response['Recommendation'] = self.recommendation_generator(response, run_config=run_config)
        
        # Flush the global handler to send data
        if hasattr(langfuse_handler, 'flush'):
            langfuse_handler.flush()
        
        return response





import os
import pandas as pd
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel
from config import configuration
from pydantic import BaseModel, Field
from typing import List, Dict, Any

# --- LANGFUSE UPDATES ---
from langfuse import Langfuse

# Set environment variables for OpenAI API
os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

# Initialize the LLM model
llm = AzureChatOpenAI(**{'deployment_name': configuration['open_ai_config']['deployment_name'], 
                         'openai_api_version': configuration['open_ai_config']['openai_api_version']}, 
                      temperature=0.1)

# Initialize Langfuse Client to handle traces manually
# This reads the env vars you set in llm.py or config
langfuse = Langfuse()

class recommend_output_format(BaseModel):
    Recommendation: list[str] = Field(description="returns a list of recommendations for improvement from the conversation between agent and customer ")


class Evaluator:
    def __init__(self):
        self.history = ""

    def calculate_timestamp_difference_tool(self, messages):
        """
        calculate the timestamp of agent and doctor/customer conversation 
        """
        df = pd.DataFrame(messages)
        if df.empty or 'timestamp' not in df.columns:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time...','description': 'Data unavailable','flag': 1}]}]

        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S:%f')
        df['timestamp_difference'] = df['timestamp'].diff().dt.total_seconds() * 1000
        df['more_than_20s'] = df['timestamp_difference'] > 20000
        
        agent_msgs = df[df['messenger']=="Agent"]
        if not agent_msgs.empty and True in set(agent_msgs['more_than_20s']):
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did exceed silent time of 20 seconds...','flag': 0}]}]
        else:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did not exceed silent time of 20 seconds...','flag': 1}]}]
    
    def load_parameters(self, sub_param_df):
        param_descriptions = {}
        for _, row in sub_param_df.iterrows():
            parameter = row['parameter_name']
            description = row['std_parameter_desc']
            if parameter not in param_descriptions:
                param_descriptions[parameter] = []
            param_descriptions[parameter].append(description)
        return param_descriptions

    # --- UPDATED: Accepts callbacks argument ---
    def recommendation_generator(self, input_json, callbacks=None):
        recommendation_template="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation with the customer. 
        
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        
        input_json:
        {input_json}
        
        format_instructions:
        {format_instructions}
        """
        recommendation_template_without_format="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation with the customer. 
        
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        4.Only give recommendations do not output the parameter name as title or heading.
        
        input_json:
        {input_json}
        """
        
        parser = JsonOutputParser(pydantic_object=recommend_output_format)
        
        prompt_recommendation = PromptTemplate(
            input_variables=["input_json"],
            partial_variables={"format_instructions": parser.get_format_instructions()},
            template=recommendation_template)
            
        prompt_recommendation_without_format = PromptTemplate(
            input_variables=["input_json"],
            template=recommendation_template_without_format)
        
        llm_chain_recommendation = prompt_recommendation | llm | parser
        llm_chain_recommendation_without_format = prompt_recommendation_without_format | llm | StrOutputParser()
        
        # --- PASSING CALLBACKS TO INVOKE ---
        config = {"callbacks": callbacks} if callbacks else {}
        
        gen_recommendations = llm_chain_recommendation.invoke({"input_json":input_json}, config=config)
        print("gen_recommendations---------", gen_recommendations)
        
        if "Recommendation" in gen_recommendations.keys():
            print("recommendation found")
            return gen_recommendations['Recommendation']
        else:
            gen_recommendations = llm_chain_recommendation_without_format.invoke({"input_json":input_json}, config=config)
            try:
                gen_recommendations=[line.split(". ",1)[1] for line in gen_recommendations.splitlines() if line.strip()]
            except:
                gen_recommendations=[line for line in gen_recommendations.splitlines() if line.strip()]
            return gen_recommendations

    # --- UPDATED: New Arguments and Trace Creation ---
    def report_generator(self, chains, conversation_history, conversation_history_list, param_descriptions, session_id=None, demo_name=None, domain=None):        
        """
        Generate a report with recommendations based on conversation history.
        Uses Langfuse Trace to link evaluation to the session.
        """
        
        # 1. CREATE A SPECIFIC TRACE FOR THIS EVALUATION
        # This links the evaluation run to your Mock Call session_id
        trace = langfuse.trace(
            name="Evaluation Report",
            session_id=session_id,
            metadata={
                "demo_name": demo_name,
                "domain": domain,
                "type": "evaluation"
            }
        )
        
        # 2. GET THE HANDLER FROM THE TRACE
        # This handler knows exactly which session it belongs to
        langfuse_handler = trace.get_langchain_handler()
        
        # Tool calculation (Manual python logic, usually not traced)
        tool_output = self.calculate_timestamp_difference_tool(conversation_history_list)
        
        map_chain = RunnableParallel(**chains)
        
        # 3. PASS HANDLER TO PARALLEL CHAIN
        response = map_chain.invoke(
            {"conversation_history": conversation_history, "tool_output": tool_output},
            config={"callbacks": [langfuse_handler]}
        )
        
        # 4. PASS HANDLER TO RECOMMENDATION
        response['Recommendation'] = self.recommendation_generator(response, callbacks=[langfuse_handler])
        
        # Ensure data is flushed to Langfuse
        langfuse.flush()
        
        return response








import os
import pandas as pd
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel
from config import configuration
from langchain_core.output_parsers import JsonOutputParser,StrOutputParser
from pydantic import BaseModel, Field
from typing import List, Dict, Any

# <--- LANGFUSE IMPORT: Import the main client, not the handler directly
from langfuse import Langfuse

# Set environment variables for OpenAI API
os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

# Initialize the LLM model
llm = AzureChatOpenAI(**{'deployment_name': configuration['open_ai_config']['deployment_name'], 
                         'openai_api_version': configuration['open_ai_config']['openai_api_version']}, 
                      temperature=0.1)

# Initialize Langfuse Client (It reads keys from your ENV or config automatically)
langfuse = Langfuse() 

class recommend_output_format(BaseModel):
    Recommendation: list[str] = Field(description="returns a list of recommendations for improvement from the conversation between agent and customer ")


class Evaluator:
    def __init__(self):
        self.history = ""

    def calculate_timestamp_difference_tool(self, messages):
        # (Your existing logic remains unchanged)
        df = pd.DataFrame(messages)
        if df.empty or 'timestamp' not in df.columns:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time...','description': 'Data unavailable','flag': 1}]}]

        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S:%f')
        df['timestamp_difference'] = df['timestamp'].diff().dt.total_seconds() * 1000
        df['more_than_20s'] = df['timestamp_difference'] > 20000
        
        agent_msgs = df[df['messenger']=="Agent"]
        if not agent_msgs.empty and True in set(agent_msgs['more_than_20s']):
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did exceed silent time of 20 seconds...','flag': 0}]}]
        else:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did not exceed silent time of 20 seconds...','flag': 1}]}]
    
    def load_parameters(self, sub_param_df):
        # (Your existing logic remains unchanged)
        param_descriptions = {}
        for _, row in sub_param_df.iterrows():
            parameter = row['parameter_name']
            description = row['std_parameter_desc']
            if parameter not in param_descriptions:
                param_descriptions[parameter] = []
            param_descriptions[parameter].append(description)
        return param_descriptions

    def recommendation_generator(self, input_json, callbacks=None):
        recommendation_template="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation with the customer. You will be given a JSON object that includes param_name with description and flag. The description explains the reason for the flag value (0 or 1). A flag of 0 means the param_name is not present in the conversation, while a flag of 1 means the param_name is present in the conversation.
        
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        
        input_json:
        {input_json}
        
        format_instructions:
        {format_instructions}
        """
        recommendation_template_without_format="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation with the customer. You will be given a JSON object that includes param_name with description and flag. The description explains the reason for the flag value (0 or 1). A flag of 0 means the param_name is not present in the conversation, while a flag of 1 means the param_name is present in the conversation.
        
        Instructions:
        1.Consider param_name value with a flag of 0 and its corresponding description into well-written English sentences as recommendation.
        2.Give recommendation in detail explaining to the agent whats needs to be done next time to improve the performance.
        3.Make sure it DO NOT exceed 15 words
        4.Only give recommendations do not output the parameter name as title or heading.
        
        input_json:
        {input_json}
        """
        
        parser = JsonOutputParser(pydantic_object=recommend_output_format)
        
        prompt_recommendation = PromptTemplate(
            input_variables=["input_json"],
            partial_variables={"format_instructions": parser.get_format_instructions()},
            template=recommendation_template)
            
        prompt_recommendation_without_format = PromptTemplate(
            input_variables=["input_json"],
            template=recommendation_template_without_format)
        
        llm_chain_recommendation = prompt_recommendation | llm | parser
        llm_chain_recommendation_without_format = prompt_recommendation_without_format | llm | StrOutputParser()
        
        # <--- TRACING: Pass callbacks config
        config = {"callbacks": callbacks} if callbacks else {}
        
        gen_recommendations = llm_chain_recommendation.invoke({"input_json":input_json}, config=config)
        print("gen_recommendations---------", gen_recommendations)
        
        if "Recommendation" in gen_recommendations.keys():
            print("recommendation found")
            return gen_recommendations['Recommendation']
        else:
            gen_recommendations = llm_chain_recommendation_without_format.invoke({"input_json":input_json}, config=config)
            try:
                gen_recommendations=[line.split(". ",1)[1] for line in gen_recommendations.splitlines() if line.strip()]
            except:
                gen_recommendations=[line for line in gen_recommendations.splitlines() if line.strip()]
            return gen_recommendations

    def report_generator(self, chains, conversation_history, conversation_history_list, param_descriptions, session_id=None, demo_name=None, domain=None):        
        """
        Generate a report with recommendations based on conversation history and dynamic parameters
        """
        
        # <--- NEW FIX: Create Trace First, Get Handler Second
        # This bypasses the constructor error because 'trace' knows how to handle session_id
        trace = langfuse.trace(
            name="evaluation_run",
            session_id=session_id,
            metadata={
                "demo_name": demo_name,
                "domain": domain
            },
            tags=[demo_name, domain, "evaluation"]
        )
        
        # Get the handler specifically for LangChain
        langfuse_handler = trace.get_langchain_handler()
        
        # Calculate tool output (Not traced by LangChain, but that's fine)
        tool_output = self.calculate_timestamp_difference_tool(conversation_history_list)
        
        map_chain = RunnableParallel(**chains)
        
        # <--- PASS HANDLER to invoke
        response = map_chain.invoke(
            {"conversation_history": conversation_history, "tool_output": tool_output},
            config={"callbacks": [langfuse_handler]}
        )
        
        # <--- PASS HANDLER to recommendation
        response['Recommendation'] = self.recommendation_generator(response, callbacks=[langfuse_handler])
        
        # Ensure everything is sent
        langfuse.flush()
        
        return response






import os
import pandas as pd
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel
from config import configuration
from langchain_core.output_parsers import JsonOutputParser,StrOutputParser
from pydantic import BaseModel, Field
from typing import List, Dict, Any

# <--- LANGFUSE IMPORT
from langfuse.callback import CallbackHandler 

# Set environment variables for OpenAI API
os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

# Initialize the LLM model
llm = AzureChatOpenAI(**{'deployment_name': configuration['open_ai_config']['deployment_name'], 
                         'openai_api_version': configuration['open_ai_config']['openai_api_version']}, 
                      temperature=0.1)

class recommend_output_format(BaseModel):
    Recommendation: list[str] = Field(description="returns a list of recommendations for improvement from the conversation between agent and customer ")


class Evaluator:
    def __init__(self):
        self.history = ""

    def calculate_timestamp_difference_tool(self,messages):
        """
        calculate the timestamp of agent and doctor/customer conversation and return a True or False if agent took more than 5 seconds to respond to the query.
        """
        # (Your existing logic remains unchanged)
        df = pd.DataFrame(messages)
        # Added safety check for empty messages
        if df.empty or 'timestamp' not in df.columns:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time...','description': 'Data unavailable','flag': 1}]}]

        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S:%f')
        df['timestamp_difference'] = df['timestamp'].diff().dt.total_seconds() * 1000
        df['more_than_20s'] = df['timestamp_difference'] > 20000
        # print(df[df['messenger']=="Agent"]['more_than_20s'].head(20))
        
        # Fixed potential error if filtering returns empty
        agent_msgs = df[df['messenger']=="Agent"]
        if not agent_msgs.empty and True in set(agent_msgs['more_than_20s']):
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did exceed silent time of 20 seconds...','flag': 0}]}]
        else:
            return [{'key': 'Call Handling','sub_parameters': [{'ID': 'CH3','param_name': 'Advocate did not exceed silent time of 20 seconds...','description': 'Advocate did not exceed silent time of 20 seconds...','flag': 1}]}]
    
    def load_parameters(self, sub_param_df):
        # (Your existing logic remains unchanged)
        param_descriptions = {}
        for _, row in sub_param_df.iterrows():
            parameter = row['parameter_name']
            description = row['std_parameter_desc']
            if parameter not in param_descriptions:
                param_descriptions[parameter] = []
            param_descriptions[parameter].append(description)
        return param_descriptions

    # <--- MODIFIED: Added 'callbacks' argument
    def recommendation_generator(self, input_json, callbacks=None):
        recommendation_template="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation with the customer...
        (Rest of your prompt...)
        input_json:
        {input_json}
        format_instructions:
        {format_instructions}
        """
        
        recommendation_template_without_format="""
        System Instructions:
        As an AI Evaluator, your task is to provide recommendations for improving the agent's conversation with the customer...
        (Rest of your prompt...)
        input_json:
        {input_json}
        """
        
        parser = JsonOutputParser(pydantic_object=recommend_output_format)
        
        prompt_recommendation = PromptTemplate(
            input_variables=["input_json"],
            partial_variables={"format_instructions": parser.get_format_instructions()},
            template=recommendation_template)
            
        prompt_recommendation_without_format = PromptTemplate(
            input_variables=["input_json"],
            template=recommendation_template_without_format)
        
        llm_chain_recommendation = prompt_recommendation | llm | parser
        llm_chain_recommendation_without_format = prompt_recommendation_without_format | llm | StrOutputParser()
        
        # <--- TRACING: Pass callbacks config to invoke
        config = {"callbacks": callbacks} if callbacks else {}
        
        gen_recommendations = llm_chain_recommendation.invoke({"input_json":input_json}, config=config)
        print("gen_recommendations---------", gen_recommendations)
        
        if "Recommendation" in gen_recommendations.keys():
            print("recommendation found")
            return gen_recommendations['Recommendation']
        else:
            # <--- TRACING: Pass callbacks here too
            gen_recommendations = llm_chain_recommendation_without_format.invoke({"input_json":input_json}, config=config)
            
            try:
                gen_recommendations=[line.split(". ",1)[1] for line in gen_recommendations.splitlines() if line.strip()]
            except:
                gen_recommendations=[line for line in gen_recommendations.splitlines() if line.strip()]
            
            return gen_recommendations

    # <--- MODIFIED: Added session_id, demo_name, domain arguments
    def report_generator(self, chains, conversation_history, conversation_history_list, param_descriptions, session_id=None, demo_name=None, domain=None):        
        """
        Generate a report with recommendations based on conversation history and dynamic parameters
        """
        
        # <--- LANGFUSE SETUP: Initialize the handler for this specific run
        langfuse_handler = CallbackHandler(
            session_id=session_id,
            user_id="evaluator_bot",
            tags=[demo_name, domain, "evaluation"],
            metadata={
                "demo_name": demo_name,
                "domain": domain,
                "type": "evaluation_run"
            }
        )
        
        # Tool execution (Latency here is instantaneous Python code, usually not traced, but the rest is)
        tool_output = self.calculate_timestamp_difference_tool(conversation_history_list)
        
        map_chain = RunnableParallel(**chains)
        
        # <--- TRACING: Pass the handler to the main parallel chain
        # This will automatically trace ALL parallel parameter evaluations
        response = map_chain.invoke(
            {"conversation_history": conversation_history, "tool_output": tool_output},
            config={"callbacks": [langfuse_handler]}
        )
        
        # <--- TRACING: Pass the handler to the recommendation generator
        # This ensures the recommendation LLM call is part of the SAME trace
        response['Recommendation'] = self.recommendation_generator(response, callbacks=[langfuse_handler])
        
        # Flush to ensure data is sent to Langfuse immediately
        langfuse_handler.flush()
        
        return response





# In main.py inside evaluate endpoint:

            # ... (your existing code loading params) ...
            
            # Generate report
            # <--- UPDATE THIS CALL
            report = evaluator.report_generator(
                chains=eval_params_loader_obj.chains,
                conversation_history=conversation_history,
                conversation_history_list=conversation_history_list,
                param_descriptions=param_descriptions,
                # New Arguments for Tracing:
                session_id=sid,
                demo_name=query.demo_name,
                domain=query.domain
            )
            
            # ... (rest of your calculation logic) ...








# agent_util.py
from google.adk.agents import LlmAgent
from google.adk.models.vertex_ai import VertexAiModel
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
import prompts  # Importing your prompts file

def create_agent_runner():
    # 1. Initialize Model
    model = VertexAiModel("gemini-1.5-pro")

    # 2. Define Sub-Agents (Using imported prompts)
    call_initiation = LlmAgent(name="call_initiation", model=model, instruction=prompts.INITIATION_INSTRUCTION)
    verification = LlmAgent(name="verification", model=model, instruction=prompts.VERIFICATION_INSTRUCTION)
    medication = LlmAgent(name="medication", model=model, instruction=prompts.MEDICATION_INSTRUCTION)
    symptom = LlmAgent(name="symptom", model=model, instruction=prompts.SYMPTOM_INSTRUCTION)
    service = LlmAgent(name="service", model=model, instruction=prompts.SERVICE_INSTRUCTION)
    closure = LlmAgent(name="closure", model=model, instruction=prompts.CLOSURE_INSTRUCTION)
    fallback = LlmAgent(name="fallback", model=model, instruction=prompts.FALLBACK_INSTRUCTION)

    # 3. Define Root Agent
    root_agent = LlmAgent(
        name="root_router",
        model=model,
        instruction=prompts.ROOT_INSTRUCTION
    )

    # 4. Create and Return Runner
    # This object encapsulates your entire 6-agent system
    runner = Runner(
        agent=root_agent,
        sub_agents=[call_initiation, verification, medication, symptom, service, closure, fallback],
        session_service=InMemorySessionService()
    )
    
    return runner
