This is a clear pivot to the specific Open Source Google Agent Development Kit (ADK) framework (Python package google-adk). This framework is distinct from the Vertex AI SDK and offers a cleaner, code-first way to build agents using standard Python classes (Agent, Tool, Instruction).
I will implement your POC in Colab Enterprise using google-adk.
Clarification & Assumptions (Standard Protocol)
 * Assumption: You want to use the Multi-Agent pattern supported by ADK (e.g., a "Router" or "Orchestrator" delegating to specialized agents), or a single robust Agent. Given your prompt ("make agent and sub agents"), I will structure this as a Primary Agent that orchestrates the flow.
 * Assumption: We will use the latest google-adk library syntax.
 * Constraint Checklist:
   * Use google-adk open source library.
   * Use Gemini (via ADK).
   * No API Keys (ADC/Environment Auth).
   * Local SQLite DB in Colab.
   * Keep original Database Schema.
   * Full Code (No shortening).
The Solution: Outreach Agent POC (Google ADK)
Copy and paste these 4 Blocks into your Colab Enterprise notebook.
Block 1: Install & Initialize (Google ADK)
We install the specific google-adk library.
# @title 1. Install Google ADK & Dependencies
# We install the open-source Agent Development Kit
%pip install --upgrade --quiet google-adk google-cloud-aiplatform cloud-sql-python-connector pymysql

# Restart kernel to ensure the new ADK library is loaded
import IPython
app = IPython.Application.instance()
app.kernel.do_shutdown(True)

# --- WAIT FOR RESTART, THEN RUN BELOW ---
import sys
import os

# Set Project ID for ADK (It uses this for Vertex AI calls)
PROJECT_ID = !gcloud config get-value project
PROJECT_ID = PROJECT_ID[0]
LOCATION = "us-central1"

print(f"✅ Google ADK Installed. Project: {PROJECT_ID}")

Block 2: Local Database & Schema (The "Backend")
We recreate your exact schema using SQLite.
# @title 2. Setup Database (Schema & Data)
import sqlite3
import json
from typing import Dict, List, Any

# --- DATABASE SETUP ---
DB_FILE = "outreach_adk.db"

def setup_local_db():
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    
    # 1. Process Maps Table (Your schema)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS process_maps (
        map_id INTEGER PRIMARY KEY AUTOINCREMENT,
        demo_name TEXT UNIQUE,
        process_json TEXT
    )
    """)
    
    # 2. Customer Profiles Table (Your schema)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS customer_profiles (
        cust_id INTEGER PRIMARY KEY AUTOINCREMENT,
        demo_name TEXT,
        customer_info TEXT,
        clinical_summary TEXT
    )
    """)
    
    # 3. Sessions Table (For Persistence)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS sessions (
        session_id TEXT PRIMARY KEY,
        current_stage TEXT,
        chat_history TEXT
    )
    """)
    
    # --- SEED DATA ---
    # We insert the Process Map defining the "Verification" and "Resolution" stages
    sample_process = {
        "stages": [
            {
                "stage_name": "Verification",
                "objective": "Verify user identity",
                "steps": [
                    {"step_code": "V1", "intent": "ask_name", "utterance": "May I have your full name?"},
                    {"step_code": "V2", "intent": "ask_dob", "utterance": "And your date of birth?"}
                ]
            },
            {
                "stage_name": "Resolution",
                "objective": "Address security upgrade",
                "steps": [
                    {"step_code": "R1", "intent": "propose_upgrade", "utterance": "I see you have the basic plan. Would you like to add cameras?"}
                ]
            }
        ]
    }
    
    sample_customer = {
        "name": "John Doe",
        "plan": "Basic Security",
        "dob": "1980-01-01"
    }
    
    try:
        cursor.execute("INSERT INTO process_maps (demo_name, process_json) VALUES (?, ?)", 
                       ("healthcare_demo", json.dumps(sample_process)))
        cursor.execute("INSERT INTO customer_profiles (demo_name, customer_info, clinical_summary) VALUES (?, ?, ?)", 
                       ("healthcare_demo", json.dumps(sample_customer), "User has basic home security."))
        conn.commit()
        print("✅ Database Seeded with Process Maps and Profiles.")
    except sqlite3.IntegrityError:
        print("ℹ️ Database already exists.")
    finally:
        conn.close()

setup_local_db()

Block 3: Tool Definitions (tools.py)
In ADK, tools are python functions. We define them here to fetch data from our local DB.
# @title 3. Define Tools (tools.py)
import sqlite3
import json

# Connection Helper
def get_db_connection():
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    return conn

# --- TOOL 1: Get Customer Context ---
def get_customer_profile_tool(demo_name: str) -> str:
    """
    Retrieves the customer's profile and clinical summary.
    Args:
        demo_name: The identifier for the demo data (e.g., 'healthcare_demo').
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT customer_info, clinical_summary FROM customer_profiles WHERE demo_name=?", (demo_name,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        return json.dumps({
            "info": json.loads(row["customer_info"]),
            "summary": row["clinical_summary"]
        })
    return "Customer not found."

# --- TOOL 2: Get Process Logic ---
def get_process_map_tool(demo_name: str) -> str:
    """
    Retrieves the strict Process Map (Stages and Steps) the agent must follow.
    Args:
        demo_name: The identifier for the demo data.
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT process_json FROM process_maps WHERE demo_name=?", (demo_name,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        return row["process_json"]
    return "Process Map not found."

print("✅ Tools Defined: get_customer_profile_tool, get_process_map_tool")

Block 4: Agent Definition (agents.py) & Execution
Here we use the google.adk library. We define a single OutreachAgent that uses the tools to navigate the process map.
# @title 4. Define Agents (agents.py) & Run POC
from google.adk import Agent
from google.adk.model import Model

# 1. Define the Agent Logic
# ADK uses a system prompt ('instruction') and a list of tools.

# We define the specific model configuration (Gemini 1.5 Pro)
gemini_model = Model(
    model_name="gemini-1.5-pro",
    project_id=PROJECT_ID,
    location=LOCATION,
    temperature=0
)

# The Instruction acts as the "Orchestrator" or "Brain"
agent_instruction = """
You are an expert Outreach Engagement Agent for Healthcare.

CORE RESPONSIBILITY:
You must strictly follow the Process Map to guide the user from 'Verification' to 'Resolution'.

OPERATIONAL STEPS:
1. CALL `get_process_map_tool` immediately to load your stages and steps.
2. CALL `get_customer_profile_tool` to understand who you are talking to.
3. Compare the user's input to your current Stage in the Process Map.
4. If the user satisfies the current step (e.g., provides a name), move to the next step.
5. If the user asks something off-topic, politely bring them back to the current step.

Output your response naturally as the agent.
"""

# 2. Create the Agent
outreach_agent = Agent(
    name="outreach_agent",
    model=gemini_model,
    instruction=agent_instruction,
    tools=[get_customer_profile_tool, get_process_map_tool]
)

print("✅ Agent 'outreach_agent' initialized with Google ADK.")

# --- 3. RUN THE POC (Chat Loop) ---
print("\n--- STARTING OUTREACH SESSION (Type 'quit' to exit) ---")
session_history = []

while True:
    user_input = input("User: ")
    if user_input.lower() in ["quit", "exit"]:
        break
    
    # ADK Agent Execution
    # We pass the input. The agent will automatically:
    # 1. Call the tools (Process Map / DB)
    # 2. Reason over the result
    # 3. Generate the response
    response = outreach_agent.run(
        input=user_input,
        history=session_history
    )
    
    # Append to history for context in next turn
    session_history.append({"role": "user", "content": user_input})
    session_history.append({"role": "model", "content": response.content})
    
    print(f"Agent: {response.content}")

This setup uses the Open Source google-adk patterns to deploy your agent logic, completely bypassing the VM/Firewall issues.
Step-by-Step AI Agent Tutorial: Build, Test, and Deploy with ADK
This video demonstrates how to build an agent using the ADK framework, similar to the implementation provided above.









# @title 1. Install & Initialize Vertex AI ADK
# We install the specific Google Cloud SDKs for Reasoning Engines
%pip install --upgrade --quiet google-cloud-aiplatform langchain-core cloud-sql-python-connector pymysql

# Restart kernel to enforce changes
import IPython
app = IPython.Application.instance()
app.kernel.do_shutdown(True)

# --- WAIT FOR RESTART, THEN RUN BELOW ---
import sys
import os
import vertexai

# Automatic Project Detection
PROJECT_ID = !gcloud config get-value project
PROJECT_ID = PROJECT_ID[0]
LOCATION = "us-central1" # Change if your region is different
STAGING_BUCKET = f"gs://{PROJECT_ID}-vertex-staging"

# Create a staging bucket if it doesn't exist (Required for Agent Deployment)
!gsutil mb -l {LOCATION} {STAGING_BUCKET} 2>/dev/null || echo "Bucket exists"

# Initialize Vertex AI
vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)

print(f"✅ Vertex AI ADK Initialized in {PROJECT_ID}")




# @title 2. Setup Database & Define Tools
import sqlite3
import json
from typing import Dict, List, Any

# --- A. DATABASE SETUP (Replicating your Schema) ---
DB_FILE = "outreach_poc.db"

def setup_local_db():
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    
    # 1. Process Maps Table
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS process_maps (
        map_id INTEGER PRIMARY KEY AUTOINCREMENT,
        demo_name TEXT UNIQUE,
        process_json TEXT
    )
    """)
    
    # 2. Customer Profiles Table
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS customer_profiles (
        cust_id INTEGER PRIMARY KEY AUTOINCREMENT,
        demo_name TEXT,
        customer_info TEXT,
        clinical_summary TEXT
    )
    """)
    
    # 3. Session/Checkpoints Table (Simple version for POC)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS sessions (
        session_id TEXT PRIMARY KEY,
        current_stage TEXT,
        chat_history TEXT
    )
    """)
    
    # --- SEED DATA (The "Process Map") ---
    # This mimics your JSON logic: Verification -> Resolution
    sample_process = {
        "stages": [
            {
                "stage_name": "Verification",
                "objective": "Verify user identity",
                "steps": [
                    {"step_code": "V1", "intent": "ask_name", "utterance": "May I have your full name?"},
                    {"step_code": "V2", "intent": "ask_dob", "utterance": "And your date of birth?"}
                ]
            },
            {
                "stage_name": "Resolution",
                "objective": "Address security upgrade",
                "steps": [
                    {"step_code": "R1", "intent": "propose_upgrade", "utterance": "I see you have the basic plan. Would you like to add cameras?"}
                ]
            }
        ]
    }
    
    sample_customer = {
        "name": "John Doe",
        "plan": "Basic Security",
        "dob": "1980-01-01"
    }
    
    try:
        cursor.execute("INSERT INTO process_maps (demo_name, process_json) VALUES (?, ?)", 
                       ("healthcare_demo", json.dumps(sample_process)))
        cursor.execute("INSERT INTO customer_profiles (demo_name, customer_info, clinical_summary) VALUES (?, ?, ?)", 
                       ("healthcare_demo", json.dumps(sample_customer), "User has basic home security."))
        conn.commit()
        print("✅ Database Seeded.")
    except sqlite3.IntegrityError:
        print("ℹ️ Database already exists.")
    finally:
        conn.close()

setup_local_db()

# --- B. TOOL DEFINITIONS (Pure Python Functions) ---
# In Vertex AI ADK, tools are just functions we bind to the agent.

def get_customer_profile(demo_name: str) -> Dict[str, Any]:
    """Fetches the customer profile and clinical summary from the database."""
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    cursor.execute("SELECT customer_info, clinical_summary FROM customer_profiles WHERE demo_name=?", (demo_name,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        return {
            "info": json.loads(row["customer_info"]),
            "summary": row["clinical_summary"]
        }
    return {"error": "Customer not found"}

def get_process_map(demo_name: str) -> Dict[str, Any]:
    """Fetches the approved process flow (stages and steps) for the agent to follow."""
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    cursor.execute("SELECT process_json FROM process_maps WHERE demo_name=?", (demo_name,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        return json.loads(row["process_json"])
    return {"error": "Process map not found"}

def log_session_state(session_id: str, stage: str, history: str):
    """Logs the current state of the conversation to the DB."""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute("INSERT OR REPLACE INTO sessions (session_id, current_stage, chat_history) VALUES (?, ?, ?)", 
                   (session_id, stage, history))
    conn.commit()
    conn.close()

print("✅ Tools Defined: get_customer_profile, get_process_map, log_session_state")


# @title 3. Define agents.py (Vertex AI Reasoning Engine)
from vertexai.preview import reasoning_engines
from langchain_google_vertexai import ChatVertexAI
import json

class OutreachAgent:
    """
    An AI Agent for Outreach Engagement in Healthcare.
    It follows a strict Process Map to guide the conversation.
    """
    
    def __init__(self, demo_name: str = "healthcare_demo"):
        self.demo_name = demo_name
        # We initialize Gemini using the Vertex AI ADK pattern
        # This model will have access to our defined tools
        self.model = reasoning_engines.Gemini(
            model="gemini-1.5-pro",
            temperature=0,
            tools=[get_customer_profile, get_process_map] 
        )

    def query(self, session_id: str, user_input: str, chat_history: list = None) -> str:
        """
        The entry point for the Agent. 
        In LangGraph, this was 'app.invoke'. In ADK, it is a method call.
        """
        if chat_history is None:
            chat_history = []
            
        # 1. Fetch Context (The "Load Context" Node)
        # We force the model to 'call' the tools to get its grounding data
        # Note: We construct a specialized prompt that ACTS as the "Router" and "Orchestrator"
        
        system_prompt = f"""
        You are an Outreach Engagement Agent acting on behalf of a Healthcare Provider.
        
        YOUR GOAL:
        Follow the strict 'Process Map' for the demo '{self.demo_name}'. 
        You must move the user through the Stages defined in that map.
        
        INSTRUCTIONS:
        1. ALWAYS call `get_process_map` first to understand your stages.
        2. ALWAYS call `get_customer_profile` to know who you are talking to.
        3. Compare the `user_input` and `chat_history` to the Process Map stages.
        4. Determine which Stage we are in (Verification, Resolution, etc.).
        5. Generate the response based strictly on the 'utterance' or 'objective' defined in that Stage.
        
        CURRENT SESSION: {session_id}
        """
        
        # 2. Invoke the Model (The "Sub-Agents" Logic)
        # In ADK, the model acts as the orchestrator. It will:
        # a) Decide to call tools (get_process_map)
        # b) Execute tools (we handle this automatically in Reasoning Engine)
        # c) Generate the final response
        
        response = self.model.query(
            prompt=system_prompt,
            messages=[
                {"role": "user", "content": user_input}
            ]
        )
        
        # 3. Persistence (The "Checkpointer")
        # We manually log the state after the turn
        log_session_state(session_id, "active", json.dumps(str(response)))
        
        return response.text

# --- 4. DEPLOYMENT (The "Managed Proof of Concept") ---
# This creates the "Reasoning Engine" in Vertex AI. 
# It bundles this Class, the Tools, and the dependencies into a managed service.

# UNCOMMENT THE LINES BELOW TO DEPLOY TO GOOGLE CLOUD (Takes ~5 mins)
# remote_agent = reasoning_engines.ReasoningEngine.create(
#     OutreachAgent(demo_name="healthcare_demo"),
#     requirements=["google-cloud-aiplatform", "langchain-core"],
#     display_name="outreach-healthcare-poc"
# )
# print(f"✅ Agent Deployed! Resource Name: {remote_agent.resource_name}")

# --- 5. LOCAL TESTING (Run in Notebook) ---
print("--- Starting Local Test ---")
local_agent = OutreachAgent(demo_name="healthcare_demo")

# Simulate Turn 1
print("\nUser: Hi, who is this?")
response_1 = local_agent.query(session_id="test_001", user_input="Hi, who is this?")
print(f"Agent: {response_1}")

# Simulate Turn 2
print("\nUser: I am John Doe.")
response_2 = local_agent.query(session_id="test_001", user_input="I am John Doe", chat_history=[response_1])
print(f"Agent: {response_2}")








This is great. Having your actual code allows us to perform a "surgical" migration. Your current setup is a classic Stateful Multi-Agent System that relies on a centralized MySQL database for its business logic (the "process maps") and in-memory persistence for short-term chat memory.
To move this to Google Cloud (Vertex AI/Cloud Run), we need to address three specific code-level shifts:
 * Database Strategy: Swapping mysql-connector for the cloud-sql-python-connector to handle GCP's networking.
 * Persistence Strategy: Moving from MemorySaver() (which wipes every time the Cloud Run instance restarts) to a Cloud SQL-backed Checkpointer for production-grade persistence.
 * Environment Parity: Moving hardcoded configurations (like PHOENIX_COLLECTOR_ENDPOINT) into environment variables.
1. Code Level: db_connection.py
We need to modify your DBConnection class to support the GCP Cloud SQL Connector. This avoids the need for IP Whitelisting or complex firewall rules.
import os
import pymysql # Use pymysql with the Cloud SQL Connector
from google.cloud.sql.connector import Connector, IPTypes
from config import configuration

class DBConnection:
    def __init__(self):
        # We now use environment variables set in the GCP Console
        self.db_type = os.getenv("DB_TYPE", "db_dev")
        self.instance_connection_name = os.getenv("INSTANCE_CONNECTION_NAME") # "project:region:instance"
        self.db_user = os.getenv("DB_USER")
        self.db_pass = os.getenv("DB_PASS")
        self.db_name = os.getenv("DB_NAME")
        
        # Local fallback logic if you are still testing on VM
        self.is_local = os.getenv("IS_LOCAL", "true").lower() == "true"
        
        self.db_enabled = True # Force enabled for cloud
        self.connector = Connector()

    def _connect(self):
        # If running in GCP, use the secure connector
        if not self.is_local:
            return self.connector.connect(
                self.instance_connection_name,
                "pymysql",
                user=self.db_user,
                password=self.db_pass,
                db=self.db_name,
                ip_type=IPTypes.PUBLIC
            )
        else:
            # Your existing local connection logic
            import mysql.connector
            config = configuration[self.db_type]
            return mysql.connector.connect(
                host=config['host'], port=config['port'],
                user=config['user'], password=config['password'],
                database=config['database']
            )

    # Note: Keep your insert(), fetch_one(), etc. identical. 
    # Just ensure you handle the cursor type correctly (pymysql vs mysql.connector).
    def fetch_one(self, query, params=()):
        conn = self._connect()
        # Handle difference in cursor initialization
        cursor = conn.cursor(pymysql.cursors.DictCursor) if not self.is_local else conn.cursor(dictionary=True)
        query = query.replace("?", "%s")
        cursor.execute(query, params)
        result = cursor.fetchone()
        conn.close()
        return result

2. Code Level: graph.py
Currently, you use MemorySaver(). In Cloud Run, instances "scale to zero" or restart. MemorySaver will lose all your chat progress.
Recommendation: Swap MemorySaver for a MySQL-based checkpointer (using your Cloud SQL instance).
# Updated get_graph function in graph.py
from langgraph.checkpoint.mysql import MySQLSaver # You'll need to install langgraph-checkpoint-mysql

def get_graph(with_memory: bool = True):
    graph = build_graph()
    
    if with_memory:
        # Check if we are in production/cloud
        if os.getenv("IS_LOCAL") == "false":
            # Use Cloud SQL for persistence
            db_conn = DBConnection()
            checkpointer = MySQLSaver(db_conn._connect()) 
            return graph.compile(checkpointer=checkpointer)
        else:
            return graph.compile(checkpointer=MemorySaver())
    return graph.compile()

3. Code Level: main.py
We need to make the API "Cloud-Native" by removing hardcoded host/port and cleaning up the Arize Phoenix setup for the cloud.
# main.py changes
import os

# 1. Dynamic Port for Cloud Run
PORT = int(os.environ.get("PORT", 7006))

# 2. Conditional Observability
if os.getenv("ENABLE_TRACING") == "true":
    from phoenix.otel import register
    register(
        project_name=os.getenv("PHOENIX_PROJECT", "healthcare-outreach"),
        endpoint=os.getenv("PHOENIX_COLLECTOR_ENDPOINT")
    )

# 3. Use the correct URL from your config
demo_endpoint = configuration['api_url']['demo_url']

@app.post(f"{demo_endpoint}/generatemockcall")
async def mock_call(query: MockQuery):
    # Your existing logic remains 95% the same!
    # The 'sessions' dict should ideally be moved to Redis or Cloud SQL 
    # if you want to share session metadata across multiple Cloud Run instances.
    ...

if __name__ == "__main__":
    # Change host to 0.0.0.0 for GCP visibility
    uvicorn.run(app, host="0.0.0.0", port=PORT)

4. The requirements.txt (The Migration "Engine")
Since you cannot use Docker locally, this file is what Google Cloud Build will use to build your environment. Add these to your existing file:
fastapi
uvicorn
langgraph
langgraph-checkpoint-mysql
langchain-openai
google-cloud-sql-connector[pymysql]
pymysql
cryptography
pydantic
arize-phoenix

Summary of what you need to do in the GCP Console:
 * Cloud SQL: Create a MySQL instance.
 * Secret Manager: (Optional) Put your OPENAI_API_KEY and DB_PASS here.
 * Deployment: Run the gcloud run deploy command I provided in the previous turn.
Next Step
Would you like me to help you write a Python script to initialize the MySQL tables specifically for the MySQLSaver checkpointer so your graph's persistence is ready before you deploy?
