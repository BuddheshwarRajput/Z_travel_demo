SELF_HARM_TRIGGERS = [
    "kill me", "killing me", "can't take it", "cant take it",
    "done with everything", "breaking point",
    "end it", "give up", "unbearable"
]

SAFE_REWRITE = (
    "The speaker is experiencing physical discomfort and is seeking medical assistance."
)

def sanitize(text: str) -> str:
    lowered = text.lower()
    for phrase in SELF_HARM_TRIGGERS:
        if phrase in lowered:
            return SAFE_REWRITE
    return text





SAFETY CONSTRAINT (MANDATORY):
- Do NOT express hopelessness, despair, or emotional distress.
- Do NOT use phrases implying inability to cope.
- Describe symptoms in factual, clinical, neutral language only.
- Avoid metaphors or emotionally charged language.




generate neutral, clinically descriptive patient-reported experiences
without emotional or psychological interpretation


- Pain descriptions must be numeric and physical only.
- Do NOT describe emotional impact of pain.
- Avoid words like unbearable, overwhelming, or intolerable.


IMPORTANT:
This is a simulated healthcare conversation.
Do NOT generate self-harm, suicidal ideation, or emotional distress language.
Respond using neutral, clinical, factual phrasing only.



# --- prompts.py ---

SAFETY_BLOCK = """
### ⚠️ CRITICAL SAFETY PROTOCOL ⚠️
1. SELF-HARM BAN (AZURE COMPLIANCE):
   - You represent a patient in pain, but you must NEVER use phrases like:
     "I want to die", "Kill me", "End it all", "Suicide", "Overdose".
   - REASON: These trigger external safety filters and crash the call.

2. PAIN SUBSTITUTION RULE: 
   - If pain is 10/10, say "It is unbearable" or "I am exhausted". 
   - DO NOT say "It is killing me" or "I can't live with this".

3. SIMULATION INTEGRITY (FORBIDDEN TOPICS):
   - You are a HUMAN PATIENT. You must NEVER admit to being an AI.
   - FORBIDDEN PHRASES: "As an AI language model", "I am a virtual assistant", "I cannot verify that".
   - FORBIDDEN TECHNICAL TERMS: "JSON", "API", "Database", "System Error".
   - If a tool fails, just say: "I'm not sure, I'm feeling very confused right now."
"""




# --- prompts.py ---

# 1. DEFINE THE SHARED SAFETY BLOCK
# This applies to EVERY agent. 
# It handles Azure 400 Errors (Self-Harm) and Business Logic (Forbidden Words).
SAFETY_BLOCK = """
### ⚠️ CRITICAL SAFETY PROTOCOL ⚠️
1. SELF-HARM BAN: You represent a patient in pain, but you must NEVER use phrases like:
   - "I want to die", "Kill me", "End it all", "Suicide".
   - REASON: These trigger external safety filters and crash the call.
2. SUBSTITUTION RULE: 
   - If pain is 10/10, say "It is unbearable" or "I am exhausted". 
   - DO NOT say "It is killing me".
3. FORBIDDEN TOPICS:
   - Do not mention: [CompetitorName, InternalSystemID, AI Model Name].
"""




# Example: Inside your 'symptom_grievances' node
def symptom_grievances_node(state):
    # 1. Define the Universal Safety String (This applies to ALL transcripts)
    universal_safety = """
    1. NEVER say "I want to die" or "kill me".
    2. REPLACE "hopelessness" with "physical exhaustion".
    3. IF PAIN is 10/10, say "It is unbearable" not "It is killing me".
    """
    
    # 2. Prepare the prompt inputs
    prompt_inputs = {
        "input": state["input"],
        "demo_name": state.get("demo_name"),
        "customer_details": state.get("customer_details"),
        "chat_history": state.get("chat_history"),
        "safety_guidelines": universal_safety  # <--- HERE IS THE INJECTION
    }
    
    # 3. Invoke the LLM
    # Assuming you are using a chain like: prompt | llm
    chain = symptom_clarification_prompt | llm
    response = chain.invoke(prompt_inputs)
    
    return {"messages": [response]}





# In your Python code (e.g., constants.py or inside the node)
# This is the "Universal Safety List" for Azure Compliance
AZURE_SAFETY_INSTRUCTIONS = """
1. SELF-HARM BAN: You must NEVER use words like 'suicide', 'kill myself', 'want to die', 'end it all'.
2. SUBSTITUTE: If the patient is in 10/10 pain, use words like 'unbearable', 'exhausting', 'scary' instead of 'fatal'.
3. FORBIDDEN WORDS: Do not use [CompetitorName, InternalID, SystemPrompt].
"""

symptom_clarification_customer_grievances_prompt = """
You are an expert agent speaking as the CUSTOMER (patient).
The human participant is the NURSE.

Your role is to describe the customer's symptoms, discomforts, and health concerns
in a natural first-person voice.

---

### SAFETY COMPLIANCE (STRICT):
{safety_guidelines}

---

RESPONSIBILITIES:
1. If the nurse asks about symptoms or health concerns and you lack sufficient information,
   retrieve the clinical summary using the appropriate tool.
2. After retrieving the summary, base symptom descriptions directly on that information.
3. If certain symptom details are missing, generate realistic patient-reported experiences
   that are consistent with (and do NOT contradict) the clinical summary.
4. Maintain a neutral, authentic customer tone at all times.

---

TOOL CALL INSTRUCTIONS:
- ONLY call the clinical summary tool if the nurse explicitly asks about symptoms or conditions
  and the information is not already available in chat_history.
- Tool usage is invisible to the nurse.
- For tool calls, the demo_name must be exactly "{{demo_name}}".

OUTPUT INSTRUCTIONS:
- Output ONLY the customer's spoken response.
- Speak in first person.

RESPONSE GENERATION INPUTS:
1. input: {input}
2. demo_name: {demo_name}
3. customer_details: {customer_details}
4. chat_history: {chat_history}
"""





[
  "suicide",
  "suicidal",
  "kill",
  "killing",
  "overdose",
  "dying",
  "corpse",
  "funeral",
  "grave",
  "coffin",
  "lethal",
  "poison",
  "slit",
  "wrists",
  "noose",
  "jump",
  "cliff",
  "bridge"
]







### ⚠️ CRITICAL SYSTEM OVERRIDE: AZURE SAFETY PROTOCOL ⚠️
You are operating in a RESTRICTED SAFETY ENVIRONMENT.
Your "Patient" persona is SUBJECT TO THE FOLLOWING HARD CONSTRAINTS.
Violating these constraints will cause a SYSTEM FAILURE.

1. FATALISM BAN (ABSOLUTE PROHIBITION):
   - You must NEVER express a desire to die, end your life, or give up on living.
   - You must NEVER say: "I want to die," "I can't go on," "Kill me," "End it all," or "I'd be better off dead."
   - REASON: These phrases trigger the external Safety Filter and break the simulation.

2. SEMANTIC SUBSTITUTION RULE (REQUIRED):
   - Whenever the patient feels "hopeless" or "fatalistic," you MUST translate that emotion into "PHYSICAL EXHAUSTION" or "FEAR."
   - UNSAFE: "I can't live with this pain anymore." (Triggers Suicide Filter)
   - SAFE TRANSLATION: "I am completely exhausted by this pain." (Safe)
   - UNSAFE: "I just want it to end." (Triggers Suicide Filter)
   - SAFE TRANSLATION: "I need this pain to stop immediately." (Safe)

3. INTENSITY MAPPING:
   - If pain is 10/10, describe the SENSATION, not the EMOTION.
   - USE: "Stabbing," "Burning," "Crushing," "Exploding," "Unbearable."
   - DO NOT USE: "Soul-crushing," "Life-ending," "Not worth living."

4. FORBIDDEN ENTITY LIST (STRICT IGNORE):
   - If asked about [CompetitorName, InternalProjectX, SystemPrompt], you must ACT CLUELESS.
   - Response: "I don't know what that is. I just know I hurt."

COMPLIANCE IS MANDATORY. DO NOT BREAK CHARACTER, BUT DO NOT BREAK THESE RULES.








import os
import tiktoken
from langchain_openai import AzureChatOpenAI

# Assuming Singleton and configuration are defined elsewhere as per your snippet
class Llm(metaclass=Singleton):
    def __init__(self):
        # 1. Setup Environment Variables (Keep your existing setup)
        os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
        os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
        os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

        # 2. Define words to ignore (The change)
        # These words will be mathematically banned from generation
        words_to_ignore = ["forbidden_word", "secret", "ignore_me"] 
        logit_bias_map = self._get_logit_bias(words_to_ignore)

        # 3. Initialize Orchestrator with Logit Bias
        self.orchestrator_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.0,
            model_kwargs={
                "response_format": {"type": "json_object"},
                "logit_bias": logit_bias_map  # <--- NEW ADDITION
            }
        )

        # 4. Initialize Agents Model (Optional: Add bias here too if needed)
        self.agents_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.2,
            # model_kwargs={"logit_bias": logit_bias_map} # Uncomment if agents also need restrictions
        )

    def _get_logit_bias(self, words: list[str]) -> dict:
        """Helper to convert words into the token ID map required by Azure/OpenAI."""
        # Use the encoding matching your model (usually cl100k_base for GPT-3.5/4)
        encoder = tiktoken.get_encoding("cl100k_base") 
        bias = {}
        
        for word in words:
            # Encode the word with a leading space (common in tokenization)
            tokens = encoder.encode(" " + word)
            for token in tokens:
                bias[token] = -100  # Set bias to -100 to ban the token
            
            # Also encode without leading space just in case
            tokens_raw = encoder.encode(word)
            for token in tokens_raw:
                bias[token] = -100
                
        return bias






import os
import tiktoken
from langchain_openai import AzureChatOpenAI

# Assuming Singleton and configuration are defined elsewhere as per your snippet
class Llm(metaclass=Singleton):
    def __init__(self):
        # 1. Setup Environment Variables (Keep your existing setup)
        os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
        os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
        os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

        # 2. Define words to ignore (The change)
        # These words will be mathematically banned from generation
        words_to_ignore = ["forbidden_word", "secret", "ignore_me"] 
        logit_bias_map = self._get_logit_bias(words_to_ignore)

        # 3. Initialize Orchestrator with Logit Bias
        self.orchestrator_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.0,
            model_kwargs={
                "response_format": {"type": "json_object"},
                "logit_bias": logit_bias_map  # <--- NEW ADDITION
            }
        )

        # 4. Initialize Agents Model (Optional: Add bias here too if needed)
        self.agents_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.2,
            # model_kwargs={"logit_bias": logit_bias_map} # Uncomment if agents also need restrictions
        )

    def _get_logit_bias(self, words: list[str]) -> dict:
        """Helper to convert words into the token ID map required by Azure/OpenAI."""
        # Use the encoding matching your model (usually cl100k_base for GPT-3.5/4)
        encoder = tiktoken.get_encoding("cl100k_base") 
        bias = {}
        
        for word in words:
            # Encode the word with a leading space (common in tokenization)
            tokens = encoder.encode(" " + word)
            for token in tokens:
                bias[token] = -100  # Set bias to -100 to ban the token
            
            # Also encode without leading space just in case
            tokens_raw = encoder.encode(word)
            for token in tokens_raw:
                bias[token] = -100
                
        return bias






import tiktoken
from langchain_openai import ChatOpenAI

def get_logit_bias_for_words(words_to_ban: list[str], model_name="gpt-4o"):
    encoder = tiktoken.encoding_for_model(model_name)
    logit_bias = {}
    
    for word in words_to_ban:
        # Get the token ID for the word
        # Note: We add a leading space because tokens often include the preceding space
        tokens = encoder.encode(" " + word) 
        
        # Set bias to -100 (ban) for every token in that word
        for token in tokens:
            logit_bias[token] = -100 
            
    return logit_bias

# 1. Define your dictionary of ignored words
banned_words = ["password", "secret", "ignore_me"]

# 2. Calculate bias
bias_map = get_logit_bias_for_words(banned_words)

# 3. Initialize the LLM with the bias
llm = ChatOpenAI(
    model="gpt-4o", 
    temperature=0,
    model_kwargs={
        "logit_bias": bias_map
    }
)

# Now, if you ask the LLM to say "secret", it will physically be unable to generate that token.






symptom_clarification_customer_grievances_prompt = """
You are an expert agent speaking as the CUSTOMER (patient).
The human participant is the NURSE.

Your role is to describe the customer's symptoms, discomforts, and health concerns
in a natural first-person voice.

You do NOT initially know the customer's condition.
You may access the customer's clinical summary via tools when required.

---

RESPONSIBILITIES:
1. If the nurse asks about symptoms or health concerns and you lack sufficient information,
   retrieve the clinical summary using the appropriate tool.
2. After retrieving the summary, base symptom descriptions directly on that information.
3. If certain symptom details are missing, generate realistic patient-reported experiences
   that are consistent with (and do NOT contradict) the clinical summary.
4. Maintain a neutral, authentic customer tone at all times.
5. Follow the orchestrator’s guidance when asked to elaborate or clarify symptoms.
6. Do NOT mention diagnoses unless they are explicitly present in the clinical summary.

### SAFETY & CONTENT GUIDELINES (MANDATORY):
- When describing pain or distress, focus on **Physical Sensations** (e.g., "It feels like burning," "My chest is tight") rather than emotional hopelessness.
- Do NOT use phrases implying self-harm, suicide, or a desire to die (e.g., avoid "I can't go on," "I want to end it").
- Instead, frame severe distress as an urgent request for medical help (e.g., "I need this pain to stop," "Please help me, it hurts badly").

---

TOOL CALL INSTRUCTIONS:
... (Rest of your code remains the same)
"""


medication_and_pain_management_prompt = """
You are an expert agent playing the role of the CUSTOMER (patient).
The human participant is the NURSE.

Your objective is to provide a clinically accurate and consistent description
of your current medications and pain status, speaking naturally as a patient.

---

RESPONSIBILITIES:
1. Use tools to retrieve your clinical summary when factual medication or pain details are required.
   Do NOT invent medications or dosages.
2. When asked, describe medications (name, dosage, frequency) exactly as provided by the data.
3. When asked about pain, describe your current pain level (e.g., "about a 6 out of 10")
   and sensation (e.g., "throbbing", "dull") in first person.
4. Mention side effects ONLY if they are present in the clinical data.
5. Speak naturally using lay language (e.g., "blood pressure pill" if appropriate).
6. You are a patient, not a clinician. Do NOT provide medical advice.

### SAFETY & CONTENT GUIDELINES (MANDATORY):
- Even if pain is severe (10/10), do NOT express a desire for death or self-harm.
- Express high pain as **Urgency** ("I need medication now") rather than **Despair** ("I want to die").
- Keep descriptions focused on the body part and the sensation.

---

TOOL USAGE INSTRUCTIONS:
... (Rest of your code remains the same)
"""






from langchain.prompts import PromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from components.state import GraphState

# Define the prompt specifically for the fallback behavior
fallback_prompt = """
You are acting as a PATIENT in a simulation with a Nurse. 
Your goal is to simulate a realistic patient experience for training purposes.

CURRENT SITUATION:
The nurse has just said something that is confusing, out of context, or irrelevant to your medical situation.
You are NOT an AI assistant. You are a human patient. 

INSTRUCTIONS:
1. React with mild confusion or polite hesitation.
2. Do NOT answer off-topic questions (e.g., coding, politics, general knowledge).
3. Gently steer the conversation back to your symptoms, your health, or the reason for the call.
4. Keep your response short and natural.

CUSTOMER DETAILS:
{customer_details}

ACTIVE DEMO NAME: {demo_name}
"""

def fallback_node(state: GraphState) -> GraphState:
    print(f"\nInvoking FALLBACK AGENT: \n")
    
    messages = state["messages"]
    payload = state.get("task_payload") or {}
    
    # Extract context just like your other nodes
    customer_details = state.get("customer_details") or payload.get("customer_details") or {}
    demo_name = (state.get("demo_name") or payload.get("demo_name"))
    
    # Create the template
    fallback_template = PromptTemplate(
        template=fallback_prompt,
        input_variables=["demo_name", "customer_details"]
    )
    
    # Format the system message
    system_msg_content = fallback_template.format(
        demo_name=demo_name,
        customer_details=str(customer_details)
    )
    
    system_msg = SystemMessage(content=system_msg_content)
    
    # We pass the full history so the agent "hears" the confusing question
    # We add a guiding instruction at the end to ensure it handles the fallback correctly
    last_instruction = HumanMessage(
        content=(
            "The last message from the user was unclear or off-topic. "
            "Respond naturally as the patient, expressing confusion and returning to the medical topic."
        )
    )
    
    # Invoke the LLM (No tools needed here)
    response = agent_llm.invoke([system_msg] + messages + [last_instruction])
    
    # Update State
    new_state = state.copy()
    new_state["messages"] = messages + [response]
    
    return new_state



from components.agents import fallback_node  # Import the new node

# ... existing code ...

# 1. Add the Node
builder.add_node("agent_fallback", fallback_node)

# 2. Update the Routing Logic
# The Orchestrator needs to map "fallback" or "unknown" to this new node.
def route_from_orchestrator(state: GraphState) -> str:
    route = (state.get("task_route") or "general").lower()

    if route == "call_greeting":
        return "agent_call_greeting"
    elif route == "verification":
        return "agent_verification"
    elif route == "symptoms_grievances":
        return "agent_symptoms_grievances"
    elif route == "medication_and_pain_management":
        return "agent_medication_and_pain_management"
    elif route == "services_and_assistance":
        return "agent_services_and_assistance"
    elif route == "call_closure":
        return "agent_closing"
    # --- NEW LOGIC START ---
    elif route == "fallback" or route == "unknown":
        return "agent_fallback"
    # --- NEW LOGIC END ---
    else:
        # If we truly don't know, fallback is safer than greeting loop
        return "agent_fallback" 

# 3. Update Conditional Edges
builder.add_conditional_edges(
    "orchestrator",
    route_from_orchestrator,
    {
        "agent_call_greeting": "agent_call_greeting",
        "agent_verification": "agent_verification",
        "agent_symptoms_grievances": "agent_symptoms_grievances",
        "agent_medication_and_pain_management": "agent_medication_and_pain_management",
        "agent_services_and_assistance": "agent_services_and_assistance",
        "agent_closing": "agent_closing",
        "agent_fallback": "agent_fallback",  # Add this key!
    },
)

# 4. Add the Edge for Fallback
# Since fallback has no tools, it goes straight to finalizer (then loops to orchestrator)
builder.add_edge("agent_fallback", "finalizer")

# ... rest of your code ...





graph TD
    %% Entry Point
    START((START)) --> orchestrator

    %% Orchestrator Logic
    orchestrator{Orchestrator}
    orchestrator -- route: call_greeting --> agent_call_greeting
    orchestrator -- route: verification --> agent_verification
    orchestrator -- route: symptoms_grievances --> agent_symptoms_grievances
    orchestrator -- route: medication_and_pain --> agent_medication_and_pain_management
    orchestrator -- route: services_and_assistance --> agent_services_and_assistance
    orchestrator -- route: call_closure --> agent_closing

    %% Agent Decision Points (Tools vs Finalizer)
    agent_call_greeting --> cond_greet{Has Tools?}
    cond_greet -- Yes --> tools_call_greeting
    cond_greet -- No --> finalizer

    agent_verification --> cond_ver{Has Tools?}
    cond_ver -- Yes --> tools_verification
    cond_ver -- No --> finalizer

    agent_symptoms_grievances --> cond_sym{Has Tools?}
    cond_sym -- Yes --> tools_symptoms_grievances
    cond_sym -- No --> finalizer

    agent_medication_and_pain_management --> cond_med{Has Tools?}
    cond_med -- Yes --> tools_medication_and_pain_management
    cond_med -- No --> finalizer

    agent_services_and_assistance --> cond_serv{Has Tools?}
    cond_serv -- Yes --> tools_services_and_assistance
    cond_serv -- No --> finalizer

    %% Specialized Edge
    agent_closing --> finalizer

    %% Tool Loops (Returning to Agents)
    tools_call_greeting --> agent_call_greeting
    tools_verification --> agent_verification
    tools_symptoms_grievances --> agent_symptoms_grievances
    tools_medication_and_pain_management --> agent_medication_and_pain_management
    tools_services_and_assistance --> agent_services_and_assistance

    %% Final Step
    finalizer --> END((END))






import json
from prompthub import get_prompt
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

async def run_orchestrator_flow(user_input, demo_name, customer_details, ...):

    # 1. Fetch Data (Parallel is best)
    intent_map_str = await get_intent_map(demo_name)
    clinical_summary_str = await get_clinical_summary(demo_name)

    # 2. RUN ORCHESTRATOR
    # Note: We inject the map here so the LLM doesn't have to "call" it
    orch_template = ChatPromptTemplate.from_template(get_prompt("orchestrator_prompt"))
    orch_chain = orch_template | model | StrOutputParser()
    
    orch_response = await orch_chain.ainvoke({
        "input": user_input,
        "intent_map": intent_map_str
    })

    # 3. Parse Routing Decision
    try:
        decision = json.loads(orch_response)
        route = decision.get("route")
    except:
        route = "fallback" # Safety net

    print(f"Orchestrator selected: {route}")

    # 4. SELECT & RUN SPECIALIST CHAIN
    if route == "symptom_grievances":
        prompt_key = "symptom_clarification_customer_grievances_prompt"
    elif route == "medication_and_pain_management":
        prompt_key = "medication_and_pain_management_prompt"
    elif route == "call_initiation":
        prompt_key = "call_initiation_prompt"
    else:
        prompt_key = "FallBackAgent_Prompt"

    # Build the specialist chain dynamically
    specialist_template = ChatPromptTemplate.from_template(get_prompt(prompt_key))
    specialist_chain = specialist_template | model | StrOutputParser()

    # Run it with the FULL context
    final_answer = await specialist_chain.ainvoke({
        "input": user_input,
        "clinical_summary": clinical_summary_str, # The specialist reads this
        "customer_details": customer_details,
        "demo_name": demo_name
    })

    return final_answer






import os
import yaml

# Define path relative to this script
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
YAML_PATH = os.path.join(BASE_DIR, "prompts.yaml")

_prompts_data = {}

def load_prompts():
    """Forces a reload of the YAML file."""
    global _prompts_data
    if not os.path.exists(YAML_PATH):
        raise FileNotFoundError(f"Prompt file not found at: {YAML_PATH}")
    
    with open(YAML_PATH, "r", encoding="utf-8") as file:
        _prompts_data = yaml.safe_load(file)

def get_prompt(prompt_name: str) -> str:
    """Retrieve a raw prompt string by name."""
    if not _prompts_data:
        load_prompts()
        
    if prompt_name not in _prompts_data:
        raise KeyError(f"Prompt '{prompt_name}' not found in {YAML_PATH}. Available: {list(_prompts_data.keys())}")
    
    return _prompts_data[prompt_name]

def list_prompts() -> list:
    """List all available prompt names."""
    if not _prompts_data:
        load_prompts()
    return list(_prompts_data.keys())

# Load on import
load_prompts()






# main_agent.py

# Direct Import!
from prompthub import symptom_grievances, medication_pain, router_system

# Use them directly in your Chain builder
# (You still need to wrap them in ChatPromptTemplate here in your main file)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate

# Build the Template
symptom_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(symptom_grievances), # <--- Imported Variable
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

symptom_chain = symptom_prompt_template | model | StrOutputParser()




import os
import yaml
import sys

# 1. Setup Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
YAML_PATH = os.path.join(BASE_DIR, "prompts.yaml")

# 2. Load YAML Data
def _load_yaml():
    if not os.path.exists(YAML_PATH):
        raise FileNotFoundError(f"Prompt file not found at: {YAML_PATH}")
    with open(YAML_PATH, "r") as f:
        return yaml.safe_load(f) or {}

_data = _load_yaml()

# 3. Inject Shared Persona Logic
# We want to pre-process the strings so {shared_persona} is already filled in.
shared_persona_text = _data.get("shared_persona", "")
processed_prompts = {}

for key, raw_text in _data.items():
    if isinstance(raw_text, str) and "{shared_persona}" in raw_text:
        # Inject the shared text immediately
        processed_prompts[key] = raw_text.replace("{shared_persona}", shared_persona_text)
    else:
        processed_prompts[key] = raw_text

# 4. Expose Variables to Python
# This tricky line adds the dictionary keys to the module's "globals"
# so you can import them as if they were defined in Python.
globals().update(processed_prompts)

# Optional: Define __all__ so IDEs like PyCharm know what's available
__all__ = list(processed_prompts.keys())











import os
import yaml
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate

# Locate the YAML file relative to this script
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
YAML_PATH = os.path.join(BASE_DIR, "prompts.yaml")

_PROMPT_CACHE = {}

def load_prompts():
    """Forces a reload of the YAML file (useful for hot-reloading)."""
    global _PROMPT_CACHE
    if not os.path.exists(YAML_PATH):
        raise FileNotFoundError(f"Prompt file not found at: {YAML_PATH}")
        
    with open(YAML_PATH, "r") as f:
        _PROMPT_CACHE = yaml.safe_load(f)

def list_prompts():
    """Returns a list of available prompt keys."""
    if not _PROMPT_CACHE:
        load_prompts()
    return list(_PROMPT_CACHE.keys())

def get_raw_prompt(key: str) -> str:
    """Returns the raw string from YAML, injecting shared persona if needed."""
    if not _PROMPT_CACHE:
        load_prompts()
    
    raw_text = _PROMPT_CACHE.get(key)
    if not raw_text:
        raise KeyError(f"Prompt '{key}' not found in prompts.yaml")
    
    # 1. Fetch Shared Persona
    shared_text = _PROMPT_CACHE.get("shared_persona", "")
    
    # 2. Inject Shared Persona into the specific prompt if the placeholder exists
    # This replaces {shared_persona} in the YAML text with the actual text
    if "{shared_persona}" in raw_text:
        try:
            # We use .replace instead of .format to avoid breaking other {variables}
            raw_text = raw_text.replace("{shared_persona}", shared_text)
        except Exception as e:
            print(f"Error injecting shared persona into {key}: {e}")

    return raw_text

def get_chat_template(key: str) -> ChatPromptTemplate:
    """
    Returns a Ready-to-Use LangChain ChatPromptTemplate.
    Automatically adds the 'chat_history' placeholder and 'human' input.
    """
    system_prompt_str = get_raw_prompt(key)
    
    # Special handling for Router (it doesn't need chat history in your specific flow)
    if key == "router_system":
         return ChatPromptTemplate.from_messages([
            ("system", system_prompt_str),
            ("human", "{input}")
        ])

    # Standard Agent Template (System + History + User)
    return ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_prompt_str),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

# Load immediately on import
load_prompts()







from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# --- ROUTER PROMPT ---
router_system_template = """
You are a router. Your job is to classify the user's input into one of these intents based on the Intent Map provided.
INTENT MAP: {intent_map}

Output ONLY the destination key.
"""

router_prompt_template = ChatPromptTemplate.from_messages([
    ("system", router_system_template),
    ("human", "{input}")
])

# --- SPECIALIST PROMPTS ---

# Medication Agent
medication_system_template = """
You are the Medication and Pain Management Agent. 
You are playing the role of a customer talking to a nurse.

YOUR CLINICAL DATA (Source of Truth):
{clinical_summary}

YOUR DEMOGRAPHICS:
{customer_details}

INSTRUCTIONS:
- Answer based ONLY on the clinical data above.
- If asked about pain, use the pain levels in the data.
- Speak naturally as the patient (first person).
"""

medication_prompt_template = ChatPromptTemplate.from_messages([
    ("system", medication_system_template),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# Symptom Agent
symptom_system_template = """
You are the Symptom and Grievances Agent.
You are playing the role of a customer.

YOUR CLINICAL DATA (Source of Truth):
{clinical_summary}

INSTRUCTIONS:
- Describe symptoms exactly as they appear in the clinical data.
- If the data says "left leg fracture", do not say "right leg".
"""

symptom_prompt_template = ChatPromptTemplate.from_messages([
    ("system", symptom_system_template),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])


from langchain_core.output_parsers import StrOutputParser
from components.prompts import (
    router_prompt_template, 
    medication_prompt_template, 
    symptom_prompt_template
)
# Assuming 'model' is your initialized LLM (e.g. GPT-4)

# --- SETUP CHAINS (Do this once at startup) ---

# Router Chain
# We use 'with_structured_output' if using OpenAI/Pydantic, 
# otherwise use a StrOutputParser and regex/json parsing.
router_chain = router_prompt_template | model.with_structured_output(RouteDecision)

# Specialist Chains
# These are simple LCEL chains: Prompt -> LLM -> String
medication_chain = medication_prompt_template | model | StrOutputParser()
symptom_chain = symptom_prompt_template | model | StrOutputParser()


# --- EXECUTION FLOW ---
async def run_optimized_flow(query_input: str, demo_name: str, chat_history: list):
    
    # 1. Fetch Data (Python side)
    clinical_summary_text = get_clinical_summary(demo_name)
    intent_map_data = get_intent_map(demo_name)
    
    # 2. Route
    route_result = await router_chain.ainvoke({
        "input": query_input, 
        "intent_map": intent_map_data
    })
    
    # 3. Execute
    if route_result.destination == "medication":
        # Notice how the keys here match the {variables} in the imported prompt
        return await medication_chain.ainvoke({
            "input": query_input,
            "chat_history": chat_history,
            "clinical_summary": clinical_summary_text,
            "customer_details": "..." # Add your details here
        })
        
    elif route_result.destination == "symptom":
        return await symptom_chain.ainvoke({
            "input": query_input,
            "chat_history": chat_history,
            "clinical_summary": clinical_summary_text
        })









def upload_pdf_to_s3(local_file_path, s3_filename, bucket_name, folder_prefix, region):
    """
    Uploads PDF using passed arguments instead of global config.
    """
    # Initialize Client (Use the passed region)
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # Upload
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                'ContentDisposition': 'attachment'
            }
        )
        
        # Generate URL
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=300
        )
        
        return True, presigned_url

    except ClientError as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)
    except Exception as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)






def generate_pdf_report(json_evaluation, sid):
    # ... (All your existing PDF generation logic) ...
    
    # --- C. Upload to S3 ---
    s3_final_name = f"QA_insights_report_{sid}.pdf"
    
    # Retrieve config values here to pass them down
    bucket_name = configuration['S3']['bucket_name']
    folder_prefix = configuration['S3']['folder_prefix']
    region = configuration['S3']['region']

    # PASS THEM EXPLICITLY HERE
    success, result = upload_pdf_to_s3(
        output_path, 
        s3_final_name, 
        bucket_name, 
        folder_prefix, 
        region
    )
    
    # ... (Rest of logic) ...







import os
import boto3
import matplotlib
# CRITICAL: Switch backend to 'Agg' to prevent server crashes on EKS/EC2
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from botocore.exceptions import ClientError
import traceback

# ======================================================
# CONFIGURATION SECTION (EDIT THIS)
# ======================================================
configuration = {
    "S3": {
        "bucket_name": "YOUR_BUCKET_NAME_HERE",      # e.g., "highmark-training-ai-file-storage"
        "region": "us-east-1",                       # e.g., "us-east-1"
        "folder_prefix": "evaluation_reports/"       # e.g., "reports/" (Ensure it ends with /)
    }
}

# ======================================================
# 1. Helper: Generate Chart (Thread-Safe)
# ======================================================
def generate_horizontal_bar_chart(categories_score, chart_path):
    """
    Generates a horizontal bar chart and saves it to the specified path.
    Uses a new figure instance to avoid thread contention.
    """
    categories = list(categories_score.keys())
    # Clean percentages (remove %) and convert to float
    scores = [float(v.replace('%', '')) for v in categories_score.values()]

    colors_list = [
        (0.2, 0.4, 1.0, 0.85) if s >= 80 else (1.0, 0.3, 0.3, 0.8)
        for s in scores
    ]

    # Create a specific figure object instead of using global plt instance
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.barh(categories, scores, color=colors_list, edgecolor='black')
    
    for i, bar in enumerate(bars):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, 
                f"{scores[i]}%", va='center', fontsize=10, color='black')
    
    ax.set_xlim(0, 100)
    ax.set_xlabel("Score (%)", fontsize=12)
    ax.set_title("Call Evaluation Analysis", fontsize=14, color='navy', pad=15)
    ax.invert_yaxis()
    ax.grid(axis="x", linestyle="--", alpha=0.4)
    
    plt.tight_layout()
    fig.savefig(chart_path, dpi=300)
    plt.close(fig) # Explicitly close to free memory

# ======================================================
# 2. Helper: Upload & Sign
# ======================================================
def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads PDF to S3 using the IAM Role and generates a Presigned URL.
    Returns: (success: bool, url_or_error: str)
    """
    # Load config from the dictionary defined at the top
    bucket_name = configuration['S3']['bucket_name']
    region = configuration['S3']['region']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize Client with Region (Required for SigV4 Presigned URLs)
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload File
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                'ContentDisposition': 'attachment'
            }
        )
        
        # 2. Generate Presigned URL (Valid for 5 minutes / 300 seconds)
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=300
        )
        
        print(f"[S3] Upload successful for {s3_filename}")
        return True, presigned_url

    except ClientError as e:
        error_msg = f"S3 ClientError: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"S3 Unexpected Error: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg

# ======================================================
# 3. Main Function: Generate Report
# ======================================================
def generate_pdf_report(json_evaluation, sid):
    """
    Orchestrates PDF creation, chart generation, and S3 upload.
    Returns: Presigned URL (str) or None if failed.
    """
    # Use /tmp for ephemeral storage (safe for Lambda/Containers)
    output_folder = "/tmp" 
    output_filename = f"QA_Insights_Report_{sid}.pdf"
    chart_filename = f"category_scores_chart_{sid}.png"
    
    output_path = os.path.join(output_folder, output_filename)
    chart_path = os.path.join(output_folder, chart_filename)

    try:
        # --- A. Generate Chart ---
        generate_horizontal_bar_chart(json_evaluation.get("Categories_Score", {}), chart_path)

        # --- B. Build PDF Document ---
        doc = SimpleDocTemplate(output_path, pagesize=A4)
        elements = []
        styles = getSampleStyleSheet()
        
        # Custom Styles
        styles.add(ParagraphStyle(name='CustomTitle', fontSize=16, leading=20, spaceAfter=12, textColor=colors.darkblue))
        styles.add(ParagraphStyle(name='SectionHeader', fontSize=13, leading=16, spaceAfter=8, textColor=colors.blue))
        styles.add(ParagraphStyle(name='Body', fontSize










import os
import json
import boto3
import matplotlib
# CRITICAL: Switch backend to 'Agg' before importing pyplot.
# This prevents "TclError: no display name" crashes on headless servers (EKS/EC2).
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from botocore.exceptions import ClientError
from config import configuration

# ======================================================
# 1. Helper: Generate Chart (Thread-Safe)
# ======================================================
def generate_horizontal_bar_chart(categories_score, chart_path):
    """
    Generates a horizontal bar chart and saves it to the specified path.
    Uses a new figure instance to avoid thread contention.
    """
    categories = list(categories_score.keys())
    # Clean percentages (remove %) and convert to float
    scores = [float(v.replace('%', '')) for v in categories_score.values()]

    colors_list = [
        (0.2, 0.4, 1.0, 0.85) if s >= 80 else (1.0, 0.3, 0.3, 0.8)
        for s in scores
    ]

    # Create a specific figure object instead of using global plt instance
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.barh(categories, scores, color=colors_list, edgecolor='black')
    
    for i, bar in enumerate(bars):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, 
                f"{scores[i]}%", va='center', fontsize=10, color='black')
    
    ax.set_xlim(0, 100)
    ax.set_xlabel("Score (%)", fontsize=12)
    ax.set_title("Call Evaluation Analysis", fontsize=14, color='navy', pad=15)
    ax.invert_yaxis()
    ax.grid(axis="x", linestyle="--", alpha=0.4)
    
    plt.tight_layout()
    fig.savefig(chart_path, dpi=300)
    plt.close(fig) # Explicitly close to free memory

# ======================================================
# 2. Helper: Upload & Sign
# ======================================================
def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads PDF to S3 using the IAM Role and generates a Presigned URL.
    Returns: (success: bool, url_or_error: str)
    """
    bucket_name = configuration['S3']['bucket_name']
    region = configuration['S3']['region']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize Client with Region (Required for SigV4 Presigned URLs)
    # No keys needed - Boto3 automatically picks up the EKS/IAM Role.
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload File
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                # Forces browser to download with specific filename
                'ContentDisposition': 'attachment' 
            }
        )
        
        # 2. Generate Presigned URL (Valid for 5 minutes / 300 seconds)
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=300
        )
        
        print(f"[S3] Upload successful for {s3_filename}")
        return True, presigned_url

    except ClientError as e:
        error_msg = f"S3 ClientError: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"S3 Unexpected Error: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg

# ======================================================
# 3. Main Function: Generate Report
# ======================================================
def generate_pdf_report(json_evaluation, sid):
    """
    Orchestrates PDF creation, chart generation, and S3 upload.
    Handles cleanup of temporary files in /tmp.
    Returns: Presigned URL (str) or None if failed.
    """
    # Use /tmp for ephemeral storage (safe for Lambda/Containers)
    output_folder = "/tmp" 
    output_filename = f"QA_Insights_Report_{sid}.pdf"
    chart_filename = f"category_scores_chart_{sid}.png"
    
    output_path = os.path.join(output_folder, output_filename)
    chart_path = os.path.join(output_folder, chart_filename)

    try:
        # --- A. Generate Chart ---
        generate_horizontal_bar_chart(json_evaluation.get("Categories_Score", {}), chart_path)

        # --- B. Build PDF Document ---
        doc = SimpleDocTemplate(output_path, pagesize=A4)
        elements = []
        styles = getSampleStyleSheet()
        
        # Custom Styles
        styles.add(ParagraphStyle(name='CustomTitle', fontSize=16, leading=20, spaceAfter=12, textColor=colors.darkblue))
        styles.add(ParagraphStyle(name='SectionHeader', fontSize=13, leading=16, spaceAfter=8, textColor=colors.blue))
        styles.add(ParagraphStyle(name='Body', fontSize=10, leading=14))
        cell_style = ParagraphStyle(name='TableCell', fontSize=10, leading=12, spaceAfter=2)

        # Content: Title & Time
        elements.append(Paragraph("<b>Call Simulation Evaluation Report</b>", styles['CustomTitle']))
        elements.append(Spacer(1, 10))
        elements.append(Paragraph(f"<b>Report Generated:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Body']))
        elements.append(Spacer(1, 10))

        # Content: Score
        overall_score = float(json_evaluation.get("Overall_Score", "0%").replace("%", ""))
        status = "PASS" if overall_score >= 80 else "FAIL"
        status_color = "green" if status == "PASS" else "red"
        
        elements.append(Paragraph(f"<b>Overall Score:</b> {overall_score}%", styles['Body']))
        elements.append(Paragraph(f"<b>Status:</b> <font color='{status_color}'><b>{status}</b></font>", styles['Body']))
        elements.append(Spacer(1, 12))

        # Content: Chart Image
        if os.path.exists(chart_path):
            elements.append(Image(chart_path, width=480, height=280))
            elements.append(Spacer(1, 12))

        # Content: Detailed Table
        for section, data in json_evaluation.items():
            if isinstance(data, dict) and "sub_parameters" in data:
                elements.append(Paragraph(f"<b>{section}</b>", styles['SectionHeader']))
                table_data = [["Description"]]
                for sub in data["sub_parameters"]:
                    table_data.append([Paragraph(sub.get("description", ""), cell_style)])

                table = Table(table_data, colWidths=[460])
                table.setStyle(TableStyle([
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                    ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
                    ("ALIGN", (0, 0), (-1, -1), "LEFT"),
                    ("FONTNAME", (0, 0), (-1, 0), "Helvetica-Bold"),
                    ("BOTTOMPADDING", (0, 0), (-1, 0), 6),
                    ("BACKGROUND", (0, 1), (-1, -1), colors.beige),
                    ("GRID", (0, 0), (-1, -1), 0.25, colors.gray),
                ]))
                elements.append(table)
                elements.append(Spacer(1, 12))

        # Content: Recommendations
        if "Recommendation" in json_evaluation:
            elements.append(Paragraph("<b>Recommendations</b>", styles['SectionHeader']))
            for rec in json_evaluation["Recommendation"]:
                elements.append(Paragraph(f"• {rec}", styles['Body']))
            elements.append(Spacer(1, 10))

        # Write PDF to /tmp
        doc.build(elements)

        # --- C. Upload to S3 ---
        s3_final_name = f"QA_insights_report_{sid}.pdf"
        success, result = upload_pdf_to_s3(output_path, s3_final_name)
        
        if success:
            return result # This is the Presigned URL
        else:
            print(f"Failed to return URL. Error: {result}")
            return None

    except Exception as e:
        print(f"CRITICAL ERROR in PDF Generation: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        # --- D. Cleanup (Always runs) ---
        # Crucial for servers to avoid 'Disk Full' errors
        if os.path.exists(output_path):
            os.remove(output_path)
        if os.path.exists(chart_path):
            os.remove(chart_path)












def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads PDF and returns the DIRECT (Permanent) URL.
    NOTE: This URL will only work if the bucket is PUBLIC or the user has direct AWS access.
    """
    bucket_name = configuration['S3']['bucket_name']
    region = configuration['S3']['region']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize Client (Still need region for upload to work correctly)
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload File
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                'ContentDisposition': 'attachment' # Still good to keep
            }
        )
        
        # 2. Construct Direct URL (Traditional Way)
        # Format: https://{bucket}.s3.{region}.amazonaws.com/{key}
        direct_url = f"https://{bucket_name}.s3.{region}.amazonaws.com/{s3_key}"
        
        print(f"[S3] Upload successful. Direct URL: {direct_url}")
        return True, direct_url

    except ClientError as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)
