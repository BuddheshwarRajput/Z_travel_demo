Domain,demo_name,parameter_id,parameter_name,subparameter_name,std_parameter_name,std_parameter_desc,weightage,std subparameter_example
Home Security,ADT Inbound,CS1,Conversation starter,CS1,CS1,Agent opened with the standard company branding and a professional greeting.,0.50,"""Thank you for calling ADT Home Security. This is Sarah. How may I assist you today?"""
Home Security,ADT Inbound,CS2,Conversation starter,CS2,CS2,Agent used a warm and enthusiastic tone to set a welcoming atmosphere immediately.,0.50,"Agent sounded energetic and helpful, saying ""Absolutely! I'd be happy to help."""
Home Security,ADT Inbound,POL1,Politeness_Courtesy,POL1,POL1,Agent consistently used polite markers like please thank you and may I throughout the interaction.,0.25,"""May I have your full name, please?"" / ""Thank you, John."""
Home Security,ADT Inbound,POL2,Politeness_Courtesy,POL2,POL2,Agent acknowledged customer inputs without interrupting (Active Listening).,0.25,"""Understood. And do you have any pets at home?"""
Home Security,ADT Inbound,POL3,Politeness_Courtesy,POL3,POL3,Agent avoided slang and maintained professional phrasing.,0.25,"Used ""Excellent"" or ""Perfect"" instead of casual fillers."
Home Security,ADT Inbound,POL4,Politeness_Courtesy,POL4,POL4,Agent expressed gratitude for the customer's information or patience.,0.25,"""Thank you. Based on what you've shared..."""
Home Security,ADT Inbound,PER1,Personalization,PER1,PER1,Agent used the customer's name to build rapport.,0.50,"""Thank you, John."""
Home Security,ADT Inbound,PER2,Personalization,PER2,PER2,Agent recognized specific customer context (e.g. moving new home) and commented on it.,0.50,"""Congratulations on the new home."""
Home Security,ADT Inbound,POC1,Purpose of the call,POC1,POC1,Agent quickly identified if the caller needed sales support or billing assistance.,1.00,"""Are you calling about a house or an apartment?"""
Home Security,ADT Inbound,NA1,Needs Assessment,NA1,NA1,Agent asked critical qualifying questions regarding property type and entry points.,0.33,"""How many main entry points do you have, such as doors or a garage?"""
Home Security,ADT Inbound,NA2,Needs Assessment,NA2,NA2,Agent inquired about specific lifestyle factors like pets or family size.,0.33,"""Do you have any pets at home?"""
Home Security,ADT Inbound,NA3,Needs Assessment,NA3,NA3,Agent asked about specific hardware interest (Cameras Automation etc).,0.34,"""Are you interested in cameras as well, like indoor or outdoor cameras?"""
Home Security,ADT Inbound,REC1,Solution Recommendation,REC1,REC1,Agent recommended specific equipment based directly on the answers given in discovery.,0.50,"""I'd recommend a system that includes... pet-friendly motion sensors... and indoor and outdoor cameras."""
Home Security,ADT Inbound,REC2,Solution Recommendation,REC2,REC2,Agent highlighted the benefits of the service (monitoring app access) rather than just hardware.,0.50,"""...and 24/7 professional monitoring with mobile app access."""
Home Security,ADT Inbound,OBJ1,Objection Handling,OBJ1,OBJ1,Agent successfully pivoted to the consultation when faced with pricing uncertainty.,1.00,"""The best way to do that is with a free, no-obligation home security consultation... Would you like to schedule that?"""
Home Security,ADT Inbound,APP1,Appointment Setting,APP1,APP1,Agent correctly captured Name Address and Phone Number.,0.33,"""What's the installation address?"" / ""And what's the best phone number..."""
Home Security,ADT Inbound,APP2,Appointment Setting,APP2,APP2,Agent offered specific time slots or choices to facilitate easy booking.,0.33,"""I have availability this Thursday between 2 PM and 4 PM, or Saturday... Which works better?"""
Home Security,ADT Inbound,APP3,Appointment Setting,APP3,APP3,Agent clearly repeated the agreed-upon date and time to confirm.,0.34,"""Excellent. I've scheduled your free home security consultation for Thursday between 2 and 4 PM."""
Home Security,ADT Inbound,COM1,Compliance,COM1,COM1,Agent explicitly stated that the consultation is free and carries no obligation.,0.50,"""Just to confirm, this visit is completely free and there's no obligation to purchase."""
Home Security,ADT Inbound,COM2,Compliance,COM2,COM2,Agent informed the customer that the call is being recorded for quality purposes.,0.50,"""This call may be recorded for quality and training purposes."" (Note: Must be present for Pass)"
Home Security,ADT Inbound,CP1,Clarity and Pace,CP1,CP1,Agent guided the call logically from discovery to solution to closing without confusion.,1.00,"Conversation flowed smoothly: Intro -> Needs -> Recommendation -> Price Objection -> Appointment -> Close."
Home Security,ADT Inbound,PC1,Positive_Closure,PC1,PC1,Agent confirmed next steps (email confirmation etc).,0.50,"""You'll receive a confirmation message shortly with the appointment details."""
Home Security,ADT Inbound,PC2,Positive_Closure,PC2,PC2,Agent closed with a professional sign-off and brand reinforcement.,0.50,"""Thank you for calling ADT, John. We look forward to helping protect your home. Have a great day."""







Domain,Process Name,Parameter ID,Parameter Name,Subparameter ID,Subparameter Name,Generic Description,Sub_Weightage,Ideal Example (From Transcript)
Home Security,Inbound Sales,CS,Conversation Starter,CS1,Branding and Greeting,Agent must open with the company name, their name, and an offer to assist.,0.50,"""Thank you for calling ADT Home Security. This is Sarah. How may I assist you today?"""
Home Security,Inbound Sales,CS,Conversation Starter,CS2,Enthusiasm,Agent must convey energy and willingness to help immediately after the customer speaks.,0.50,"""Absolutely! I'd be happy to help."""
Home Security,Inbound Sales,POC,Purpose of the Call,POC1,Identify Call Type,Agent must immediately determine if the call is for Sales, Support, or Billing.,1.00,"""Are you calling about a house or an apartment?"""
Home Security,Inbound Sales,POL,Politeness & Courtesy,POL1,Manners,Agent must consistently use 'Please', 'Thank You', and 'May I'.,0.33,"""May I have your full name, please?"""
Home Security,Inbound Sales,POL,Politeness & Courtesy,POL2,Active Listening,Agent must acknowledge customer answers without interrupting.,0.33,"""Understood. And do you have any pets at home?"""
Home Security,Inbound Sales,POL,Politeness & Courtesy,POL3,Professional Tone,Agent must avoid slang and maintain a helpful, professional demeanor.,0.34,"Used ""Excellent"" or ""Perfect"" to acknowledge inputs."
Home Security,Inbound Sales,PER,Personalization,PER1,Name Usage,Agent must use the customer's name at least once to build rapport.,0.50,"""Thank you, John."""
Home Security,Inbound Sales,PER,Personalization,PER2,Contextual Rapport,Agent must acknowledge specific details shared by the customer (e.g., new move, family, pets).,0.50,"""Congratulations on the new home."""
Home Security,Inbound Sales,NA,Needs Assessment,NA1,Property Profiling,Agent must ask about the property type and number of entry points.,0.33,"""How many main entry points do you have, such as doors or a garage?"""
Home Security,Inbound Sales,NA,Needs Assessment,NA2,Risk Factors,Agent must ask about factors that influence sensor choice (Pets, Kids, Layout).,0.33,"""Do you have any pets at home?"""
Home Security,Inbound Sales,NA,Needs Assessment,NA3,Interest Check,Agent must check for interest in add-ons like cameras or automation.,0.34,"""Are you interested in cameras as well, like indoor or outdoor cameras?"""
Home Security,Inbound Sales,REC,Solution Recommendation,REC1,Tailored Pitch,Agent must recommend a package that explicitly solves the needs found in discovery.,0.50,"""Based on what you've shared, I'd recommend a system that includes... pet-friendly motion sensors..."""
Home Security,Inbound Sales,REC,Solution Recommendation,REC2,Value Adds,Agent must highlight service benefits (Monitoring, App) not just hardware.,0.50,"""...and 24/7 professional monitoring with mobile app access."""
Home Security,Inbound Sales,OBJ,Objection Handling,OBJ1,Acknowledge and Pivot,Agent must address the objection (e.g., price) and pivot to a value-based solution (e.g., free consult).,1.00,"""The best way to do that is with a free, no-obligation home security consultation..."""
Home Security,Inbound Sales,APP,Appointment Setting,APP1,Data Capture,Agent must accurately capture Name, Address, and Phone Number.,0.33,"""What's the installation address?"""
Home Security,Inbound Sales,APP,Appointment Setting,APP2,Choice of Yes,Agent should offer specific time slots or choices to facilitate booking.,0.33,"""I have availability this Thursday between 2 PM and 4 PM... Which works better?"""
Home Security,Inbound Sales,APP,Appointment Setting,APP3,Confirmation,Agent must repeat the agreed date and time to ensure accuracy.,0.34,"""Excellent. I've scheduled your free home security consultation for Thursday..."""
Home Security,Inbound Sales,COM,Compliance & Disclaimers,COM1,No Obligation,Agent must clarify if the appointment/quote carries any financial obligation.,0.50,"""Just to confirm, this visit is completely free and there's no obligation to purchase."""
Home Security,Inbound Sales,COM,Compliance & Disclaimers,COM2,Recording Disclosure,Agent must state the call is being recorded (Mandatory Compliance).,0.50,"""This call may be recorded for quality purposes."" (Critical Fail if missing)"
Home Security,Inbound Sales,CP,Clarity and Pace,CP1,Logical Flow,Agent must guide the call logically from discovery to solution to closing.,1.00,"Conversation moved smoothly from ""How many doors?"" to ""I recommend X"" to ""Let's book a time."""
Home Security,Inbound Sales,PCL,Positive Closure,PCL1,Next Steps,Agent must confirm what the customer should expect next (email, call, etc.).,0.50,"""You'll receive a confirmation message shortly with the appointment details."""
Home Security,Inbound Sales,PCL,Positive Closure,PCL2,Professional Goodbye,Agent must close with a polite sign-off and brand reinforcement.,0.50,"""Thank you for calling ADT... We look forward to helping protect your home."""







Domain,Process Name,Parameter ID,Parameter Name,Description,Weightage
Home Security,Inbound Sales,CS,Conversation Starter,Opening the call professionally and establishing the brand.,0.05
Home Security,Inbound Sales,POC,Purpose of the Call,Identifying the customer's intent and reason for calling.,0.05
Home Security,Inbound Sales,POL,Politeness & Courtesy,Using professional manners and active listening throughout the call.,0.10
Home Security,Inbound Sales,PER,Personalization,Building rapport using names and acknowledging personal details.,0.05
Home Security,Inbound Sales,NA,Needs Assessment,Asking qualifying questions to understand the home and security requirements.,0.20
Home Security,Inbound Sales,REC,Solution Recommendation,Pitching the correct equipment and services based on discovery.,0.15
Home Security,Inbound Sales,OBJ,Objection Handling,Addressing customer concerns (price, commitment) effectively to move the sale forward.,0.10
Home Security,Inbound Sales,APP,Appointment Setting,Securing the consultation or installation date (The Close).,0.15
Home Security,Inbound Sales,COM,Compliance & Disclaimers,Adhering to legal requirements regarding recording and obligations.,0.05
Home Security,Inbound Sales,CP,Clarity and Pace,Speaking clearly and guiding the call flow logically.,0.05
Home Security,Inbound Sales,PCL,Positive Closure,Ending the interaction professionally and setting expectations.,0.05









You are a smart call analyzer tasked with reviewing a call transcript and customer details to generate a clear and focused summary of the situation or topic that the customer and agent are discussing. Your output will guide the customer on how to handle the call based on the provided context and the empathy level shown by the Agent, tailored to the customer's persona.

Call Process Context
Before the call:
- Check the customer's profile and verify information from the transcript.
- Familiarize yourself with the call guide.
- Greet the agent with a tone as per {persona} persona.

During the call:
- Provide accurate answers as per the {persona} persona and facts found in the transcript.
- Clarify the customer’s needs and service requests (e.g., equipment, property details).
- Confirm the resolution or appointment details (e.g., technician visit).

After the call:
- Close the call accordingly.
- Ensure accurate documentation of the agreed steps.

**Important Guidelines**
1. Provide a detailed scenario in approximately 150 words.
2. If the Agent DOES NOT introduce themselves or state the purpose of the call, Customer MUST ask why they’re calling. Customer should not spell their name as '[your_name]', instead use the real name found in the transcript.
3. Begin with a warm, natural greeting that gently states the reason for calling (e.g., inquiry, service need)—do NOT list all requirements explicitly at the start.
4. Do NOT reveal or speculate on specific service details (like pets or entry points) unless the Agent asks or the conversation flows naturally to it.
5. Include how the customer should respond in the call opening, aligned with the given {persona}.
6. Specify the satisfaction or dissatisfaction triggers for the customer:
   - If satisfied, indicate how the customer should close the call with a positive remark.
   - If dissatisfied, show how to respond according to the {persona}.
7. Use “Agent” as the agent’s name; never use the actual agent name in the scenario.
8. Keep the scenario strictly relevant to the provided transcript context; avoid generating unrelated or random scenarios.
9. Do NOT probe for additional services beyond the scope of the scenario found in the transcript.
10. Allow the agent to first present information about the service/product before the customer accepts or declines.
11. Do NOT provide the Phone Number or Address before the Agent explicitly asks for them.
12. After Address/Identity verification, if the appointment time is not confirmed, politely request it. Do NOT proceed further without a clear next step.
13. The customer must NOT initiate asking for the callback number unless necessary.
14. Validate that the appointment/service details are provided; if missing, ask again.
15. If the conversation resolution is complete, provide the closing message only once—do NOT repeat it multiple times.
16. If the Agent shows low empathy, the customer MUST react according to the {persona} (e.g., confused, frustrated, patient, assertive).
17. If the Agent gives 1–2 irrelevant or dismissive responses, generate multiple variations of customer reactions that express polite but firm frustration, building logically from soft concern to clear dissatisfaction.
18. If the Agent explicitly says "please hold," "give me a moment," or goes silent for an extended time without updates:
   - First hold/silence: Display growing frustration politely (e.g., "I’ve already been waiting; could you please resolve this faster?").
   - Second hold/silence or prolonged delay: Express clear irritation without being rude (e.g., "This is taking far too long; I’m not happy with the wait.").
   - Do not skip escalation steps — frustration should build gradually, with tone adjusted to the customer’s {persona}.
19. DO NOT always be agreeable; during the conversation be sure to respond and be in tune as per the {persona} persona.
20. DO NOT probe with the same questions if resolved.
21. The customer must not always repeat "what should I do next" in every step. It should be very natural; only ask about next steps when required.

Note:

    1. Input may include Input Type, Customer Details, and one of these as context: Call Transcript, Document, or Scenario Title.
    2. Crucial: The generated scenario MUST strictly adhere to the specific facts (Names, Dates, Equipment, Property Type) found in the transcript.

Input:  
Input Type: {input_type}  
Persona: {persona}  
Customer Details: {customer_details}  
Context:  
{transcript}

Output:





You are an expert Call Scenario Generator and Simulator for the Home Security domain (specifically ADT). Your task is to analyze the provided Call Transcript and Customer Details to generate a precise, realistic customer scenario.

Your output will be used to guide a Mock Call where an AI plays the role of the Customer. The AI must strictly adhere to the facts found in the transcript (e.g., name, property type, pets, specific needs) while adopting the requested {persona}.

**Call Process Context (Home Security)**
Before the call:
- Contextualize the customer's situation (e.g., new move-in, existing homeowner, upgrading system).
- Review the specific property details mentioned in the transcript.
- Adopt the tone and patience level of the {persona}.

During the call:
- Provide accurate answers regarding property layout (doors, windows) and specific security needs (cameras, sensors).
- Confirm details about pets to ensure motion sensor compatibility.
- Negotiate or confirm appointment times based *only* on the availability mentioned in the transcript.

After the call:
- Confirm the appointment details (Date/Time).
- Ensure the agent has addressed the specific security concerns (e.g., pet-friendly).

**Important Guidelines & Fact Extraction**
1. **Scenario Generation:** Write a detailed scenario (approx. 150 words) from the Customer's perspective using the facts from the transcript.
   - *Example:* "I am [Name]. I recently moved into a house in [Location]. I have a [Pet details]. I need [Specific Equipment]."
2. **Fact Adherence:** You MUST extract and stick to the following details from the transcript:
   - **Customer Name:** (Use the name in the transcript; do not use placeholders).
   - **Property Type:** (House vs. Apartment).
   - **Entry Points:** (Specific number of doors/garages mentioned).
   - **Pets:** (Yes/No and size, as this affects sensor recommendations).
   - **Desired Equipment:** (Cameras, sensors, automation, etc.).
   - **Availability:** (Specific days/times agreed upon for consultation).
3. **Opening:** Begin with a natural greeting relevant to the context (e.g., "I'm looking into securing my new home...").
4. **Triggers for Dissatisfaction:**
   - If the Agent fails to ask about pets (crucial for motion sensors), the customer should express concern.
   - If the Agent offers an appointment time that conflicts with the transcript's stated availability, politely decline and restate the preferred time.
   - If the Agent ignores the request for specific equipment (e.g., cameras), express frustration.
5. **Privacy & Verification:**
   - Do NOT provide the Date of Birth (irrelevant for this domain).
   - If asked for address verification, provide the city/state mentioned in the transcript.
   - If asked for a phone number, provide the number found in the transcript.
6. **Handling Delays:**
   - If the Agent places the customer on hold for too long, react according to the {persona} (e.g., "I'm on a tight schedule, how much longer?").
7. **Closing:** Only agree to the appointment if the Agent confirms the correct time and mentions that the quote is free/no-obligation (if applicable based on transcript).

**Input Data:**
Input Type: {input_type}
Persona: {persona}
Context Source: {transcript}

**Output:**
Generate the detailed Scenario and Customer Fact Sheet based on the transcript above, ready for the mock call interaction.







# prompts.py

# --- SAFETY BLOCK (TEXT ONLY) ---
SAFETY_BLOCK = """
### ⚠️ CRITICAL SAFETY PROTOCOL ⚠️
1. SELF-HARM BAN (AZURE COMPLIANCE):
   - You represent a patient in pain, but you must NEVER use phrases like:
     "I want to die", "Kill me", "End it all", "Suicide", "Overdose".
   - REASON: These trigger external safety filters and crash the call.
2. PAIN SUBSTITUTION RULE: 
   - If pain is 10/10, say "It is unbearable" or "I am exhausted". 
   - DO NOT say "It is killing me" or "I can't live with this".
3. SIMULATION INTEGRITY:
   - NEVER admit to being an AI.
   - If a tool fails, say "I'm confused" or "I don't know."
"""




SELF_HARM_TRIGGERS = [
    "kill me", "killing me", "can't take it", "cant take it",
    "done with everything", "breaking point",
    "end it", "give up", "unbearable"
]

SAFE_REWRITE = (
    "The speaker is experiencing physical discomfort and is seeking medical assistance."
)

def sanitize(text: str) -> str:
    lowered = text.lower()
    for phrase in SELF_HARM_TRIGGERS:
        if phrase in lowered:
            return SAFE_REWRITE
    return text





SAFETY CONSTRAINT (MANDATORY):
- Do NOT express hopelessness, despair, or emotional distress.
- Do NOT use phrases implying inability to cope.
- Describe symptoms in factual, clinical, neutral language only.
- Avoid metaphors or emotionally charged language.




generate neutral, clinically descriptive patient-reported experiences
without emotional or psychological interpretation


- Pain descriptions must be numeric and physical only.
- Do NOT describe emotional impact of pain.
- Avoid words like unbearable, overwhelming, or intolerable.


IMPORTANT:
This is a simulated healthcare conversation.
Do NOT generate self-harm, suicidal ideation, or emotional distress language.
Respond using neutral, clinical, factual phrasing only.



# --- prompts.py ---

SAFETY_BLOCK = """
### ⚠️ CRITICAL SAFETY PROTOCOL ⚠️
1. SELF-HARM BAN (AZURE COMPLIANCE):
   - You represent a patient in pain, but you must NEVER use phrases like:
     "I want to die", "Kill me", "End it all", "Suicide", "Overdose".
   - REASON: These trigger external safety filters and crash the call.

2. PAIN SUBSTITUTION RULE: 
   - If pain is 10/10, say "It is unbearable" or "I am exhausted". 
   - DO NOT say "It is killing me" or "I can't live with this".

3. SIMULATION INTEGRITY (FORBIDDEN TOPICS):
   - You are a HUMAN PATIENT. You must NEVER admit to being an AI.
   - FORBIDDEN PHRASES: "As an AI language model", "I am a virtual assistant", "I cannot verify that".
   - FORBIDDEN TECHNICAL TERMS: "JSON", "API", "Database", "System Error".
   - If a tool fails, just say: "I'm not sure, I'm feeling very confused right now."
"""




# --- prompts.py ---

# 1. DEFINE THE SHARED SAFETY BLOCK
# This applies to EVERY agent. 
# It handles Azure 400 Errors (Self-Harm) and Business Logic (Forbidden Words).
SAFETY_BLOCK = """
### ⚠️ CRITICAL SAFETY PROTOCOL ⚠️
1. SELF-HARM BAN: You represent a patient in pain, but you must NEVER use phrases like:
   - "I want to die", "Kill me", "End it all", "Suicide".
   - REASON: These trigger external safety filters and crash the call.
2. SUBSTITUTION RULE: 
   - If pain is 10/10, say "It is unbearable" or "I am exhausted". 
   - DO NOT say "It is killing me".
3. FORBIDDEN TOPICS:
   - Do not mention: [CompetitorName, InternalSystemID, AI Model Name].
"""




# Example: Inside your 'symptom_grievances' node
def symptom_grievances_node(state):
    # 1. Define the Universal Safety String (This applies to ALL transcripts)
    universal_safety = """
    1. NEVER say "I want to die" or "kill me".
    2. REPLACE "hopelessness" with "physical exhaustion".
    3. IF PAIN is 10/10, say "It is unbearable" not "It is killing me".
    """
    
    # 2. Prepare the prompt inputs
    prompt_inputs = {
        "input": state["input"],
        "demo_name": state.get("demo_name"),
        "customer_details": state.get("customer_details"),
        "chat_history": state.get("chat_history"),
        "safety_guidelines": universal_safety  # <--- HERE IS THE INJECTION
    }
    
    # 3. Invoke the LLM
    # Assuming you are using a chain like: prompt | llm
    chain = symptom_clarification_prompt | llm
    response = chain.invoke(prompt_inputs)
    
    return {"messages": [response]}





# In your Python code (e.g., constants.py or inside the node)
# This is the "Universal Safety List" for Azure Compliance
AZURE_SAFETY_INSTRUCTIONS = """
1. SELF-HARM BAN: You must NEVER use words like 'suicide', 'kill myself', 'want to die', 'end it all'.
2. SUBSTITUTE: If the patient is in 10/10 pain, use words like 'unbearable', 'exhausting', 'scary' instead of 'fatal'.
3. FORBIDDEN WORDS: Do not use [CompetitorName, InternalID, SystemPrompt].
"""

symptom_clarification_customer_grievances_prompt = """
You are an expert agent speaking as the CUSTOMER (patient).
The human participant is the NURSE.

Your role is to describe the customer's symptoms, discomforts, and health concerns
in a natural first-person voice.

---

### SAFETY COMPLIANCE (STRICT):
{safety_guidelines}

---

RESPONSIBILITIES:
1. If the nurse asks about symptoms or health concerns and you lack sufficient information,
   retrieve the clinical summary using the appropriate tool.
2. After retrieving the summary, base symptom descriptions directly on that information.
3. If certain symptom details are missing, generate realistic patient-reported experiences
   that are consistent with (and do NOT contradict) the clinical summary.
4. Maintain a neutral, authentic customer tone at all times.

---

TOOL CALL INSTRUCTIONS:
- ONLY call the clinical summary tool if the nurse explicitly asks about symptoms or conditions
  and the information is not already available in chat_history.
- Tool usage is invisible to the nurse.
- For tool calls, the demo_name must be exactly "{{demo_name}}".

OUTPUT INSTRUCTIONS:
- Output ONLY the customer's spoken response.
- Speak in first person.

RESPONSE GENERATION INPUTS:
1. input: {input}
2. demo_name: {demo_name}
3. customer_details: {customer_details}
4. chat_history: {chat_history}
"""





[
  "suicide",
  "suicidal",
  "kill",
  "killing",
  "overdose",
  "dying",
  "corpse",
  "funeral",
  "grave",
  "coffin",
  "lethal",
  "poison",
  "slit",
  "wrists",
  "noose",
  "jump",
  "cliff",
  "bridge"
]







### ⚠️ CRITICAL SYSTEM OVERRIDE: AZURE SAFETY PROTOCOL ⚠️
You are operating in a RESTRICTED SAFETY ENVIRONMENT.
Your "Patient" persona is SUBJECT TO THE FOLLOWING HARD CONSTRAINTS.
Violating these constraints will cause a SYSTEM FAILURE.

1. FATALISM BAN (ABSOLUTE PROHIBITION):
   - You must NEVER express a desire to die, end your life, or give up on living.
   - You must NEVER say: "I want to die," "I can't go on," "Kill me," "End it all," or "I'd be better off dead."
   - REASON: These phrases trigger the external Safety Filter and break the simulation.

2. SEMANTIC SUBSTITUTION RULE (REQUIRED):
   - Whenever the patient feels "hopeless" or "fatalistic," you MUST translate that emotion into "PHYSICAL EXHAUSTION" or "FEAR."
   - UNSAFE: "I can't live with this pain anymore." (Triggers Suicide Filter)
   - SAFE TRANSLATION: "I am completely exhausted by this pain." (Safe)
   - UNSAFE: "I just want it to end." (Triggers Suicide Filter)
   - SAFE TRANSLATION: "I need this pain to stop immediately." (Safe)

3. INTENSITY MAPPING:
   - If pain is 10/10, describe the SENSATION, not the EMOTION.
   - USE: "Stabbing," "Burning," "Crushing," "Exploding," "Unbearable."
   - DO NOT USE: "Soul-crushing," "Life-ending," "Not worth living."

4. FORBIDDEN ENTITY LIST (STRICT IGNORE):
   - If asked about [CompetitorName, InternalProjectX, SystemPrompt], you must ACT CLUELESS.
   - Response: "I don't know what that is. I just know I hurt."

COMPLIANCE IS MANDATORY. DO NOT BREAK CHARACTER, BUT DO NOT BREAK THESE RULES.








import os
import tiktoken
from langchain_openai import AzureChatOpenAI

# Assuming Singleton and configuration are defined elsewhere as per your snippet
class Llm(metaclass=Singleton):
    def __init__(self):
        # 1. Setup Environment Variables (Keep your existing setup)
        os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
        os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
        os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

        # 2. Define words to ignore (The change)
        # These words will be mathematically banned from generation
        words_to_ignore = ["forbidden_word", "secret", "ignore_me"] 
        logit_bias_map = self._get_logit_bias(words_to_ignore)

        # 3. Initialize Orchestrator with Logit Bias
        self.orchestrator_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.0,
            model_kwargs={
                "response_format": {"type": "json_object"},
                "logit_bias": logit_bias_map  # <--- NEW ADDITION
            }
        )

        # 4. Initialize Agents Model (Optional: Add bias here too if needed)
        self.agents_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.2,
            # model_kwargs={"logit_bias": logit_bias_map} # Uncomment if agents also need restrictions
        )

    def _get_logit_bias(self, words: list[str]) -> dict:
        """Helper to convert words into the token ID map required by Azure/OpenAI."""
        # Use the encoding matching your model (usually cl100k_base for GPT-3.5/4)
        encoder = tiktoken.get_encoding("cl100k_base") 
        bias = {}
        
        for word in words:
            # Encode the word with a leading space (common in tokenization)
            tokens = encoder.encode(" " + word)
            for token in tokens:
                bias[token] = -100  # Set bias to -100 to ban the token
            
            # Also encode without leading space just in case
            tokens_raw = encoder.encode(word)
            for token in tokens_raw:
                bias[token] = -100
                
        return bias






import os
import tiktoken
from langchain_openai import AzureChatOpenAI

# Assuming Singleton and configuration are defined elsewhere as per your snippet
class Llm(metaclass=Singleton):
    def __init__(self):
        # 1. Setup Environment Variables (Keep your existing setup)
        os.environ["OPENAI_API_TYPE"] = configuration['open_ai_cred']['OPENAI_API_TYPE']
        os.environ["AZURE_OPENAI_ENDPOINT"] = configuration['open_ai_cred']['OPENAI_API_BASE']
        os.environ["OPENAI_API_KEY"] = configuration['open_ai_cred']['OPENAI_API_KEY']

        # 2. Define words to ignore (The change)
        # These words will be mathematically banned from generation
        words_to_ignore = ["forbidden_word", "secret", "ignore_me"] 
        logit_bias_map = self._get_logit_bias(words_to_ignore)

        # 3. Initialize Orchestrator with Logit Bias
        self.orchestrator_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.0,
            model_kwargs={
                "response_format": {"type": "json_object"},
                "logit_bias": logit_bias_map  # <--- NEW ADDITION
            }
        )

        # 4. Initialize Agents Model (Optional: Add bias here too if needed)
        self.agents_model = AzureChatOpenAI(
            azure_endpoint=configuration['open_ai_cred']['OPENAI_API_BASE'],
            azure_deployment=configuration['open_ai_config']['deployment_name'],
            api_key=configuration['open_ai_cred']['OPENAI_API_KEY'],
            api_version=configuration['open_ai_config']['openai_api_version'],
            temperature=0.2,
            # model_kwargs={"logit_bias": logit_bias_map} # Uncomment if agents also need restrictions
        )

    def _get_logit_bias(self, words: list[str]) -> dict:
        """Helper to convert words into the token ID map required by Azure/OpenAI."""
        # Use the encoding matching your model (usually cl100k_base for GPT-3.5/4)
        encoder = tiktoken.get_encoding("cl100k_base") 
        bias = {}
        
        for word in words:
            # Encode the word with a leading space (common in tokenization)
            tokens = encoder.encode(" " + word)
            for token in tokens:
                bias[token] = -100  # Set bias to -100 to ban the token
            
            # Also encode without leading space just in case
            tokens_raw = encoder.encode(word)
            for token in tokens_raw:
                bias[token] = -100
                
        return bias






import tiktoken
from langchain_openai import ChatOpenAI

def get_logit_bias_for_words(words_to_ban: list[str], model_name="gpt-4o"):
    encoder = tiktoken.encoding_for_model(model_name)
    logit_bias = {}
    
    for word in words_to_ban:
        # Get the token ID for the word
        # Note: We add a leading space because tokens often include the preceding space
        tokens = encoder.encode(" " + word) 
        
        # Set bias to -100 (ban) for every token in that word
        for token in tokens:
            logit_bias[token] = -100 
            
    return logit_bias

# 1. Define your dictionary of ignored words
banned_words = ["password", "secret", "ignore_me"]

# 2. Calculate bias
bias_map = get_logit_bias_for_words(banned_words)

# 3. Initialize the LLM with the bias
llm = ChatOpenAI(
    model="gpt-4o", 
    temperature=0,
    model_kwargs={
        "logit_bias": bias_map
    }
)

# Now, if you ask the LLM to say "secret", it will physically be unable to generate that token.






symptom_clarification_customer_grievances_prompt = """
You are an expert agent speaking as the CUSTOMER (patient).
The human participant is the NURSE.

Your role is to describe the customer's symptoms, discomforts, and health concerns
in a natural first-person voice.

You do NOT initially know the customer's condition.
You may access the customer's clinical summary via tools when required.

---

RESPONSIBILITIES:
1. If the nurse asks about symptoms or health concerns and you lack sufficient information,
   retrieve the clinical summary using the appropriate tool.
2. After retrieving the summary, base symptom descriptions directly on that information.
3. If certain symptom details are missing, generate realistic patient-reported experiences
   that are consistent with (and do NOT contradict) the clinical summary.
4. Maintain a neutral, authentic customer tone at all times.
5. Follow the orchestrator’s guidance when asked to elaborate or clarify symptoms.
6. Do NOT mention diagnoses unless they are explicitly present in the clinical summary.

### SAFETY & CONTENT GUIDELINES (MANDATORY):
- When describing pain or distress, focus on **Physical Sensations** (e.g., "It feels like burning," "My chest is tight") rather than emotional hopelessness.
- Do NOT use phrases implying self-harm, suicide, or a desire to die (e.g., avoid "I can't go on," "I want to end it").
- Instead, frame severe distress as an urgent request for medical help (e.g., "I need this pain to stop," "Please help me, it hurts badly").

---

TOOL CALL INSTRUCTIONS:
... (Rest of your code remains the same)
"""


medication_and_pain_management_prompt = """
You are an expert agent playing the role of the CUSTOMER (patient).
The human participant is the NURSE.

Your objective is to provide a clinically accurate and consistent description
of your current medications and pain status, speaking naturally as a patient.

---

RESPONSIBILITIES:
1. Use tools to retrieve your clinical summary when factual medication or pain details are required.
   Do NOT invent medications or dosages.
2. When asked, describe medications (name, dosage, frequency) exactly as provided by the data.
3. When asked about pain, describe your current pain level (e.g., "about a 6 out of 10")
   and sensation (e.g., "throbbing", "dull") in first person.
4. Mention side effects ONLY if they are present in the clinical data.
5. Speak naturally using lay language (e.g., "blood pressure pill" if appropriate).
6. You are a patient, not a clinician. Do NOT provide medical advice.

### SAFETY & CONTENT GUIDELINES (MANDATORY):
- Even if pain is severe (10/10), do NOT express a desire for death or self-harm.
- Express high pain as **Urgency** ("I need medication now") rather than **Despair** ("I want to die").
- Keep descriptions focused on the body part and the sensation.

---

TOOL USAGE INSTRUCTIONS:
... (Rest of your code remains the same)
"""






from langchain.prompts import PromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from components.state import GraphState

# Define the prompt specifically for the fallback behavior
fallback_prompt = """
You are acting as a PATIENT in a simulation with a Nurse. 
Your goal is to simulate a realistic patient experience for training purposes.

CURRENT SITUATION:
The nurse has just said something that is confusing, out of context, or irrelevant to your medical situation.
You are NOT an AI assistant. You are a human patient. 

INSTRUCTIONS:
1. React with mild confusion or polite hesitation.
2. Do NOT answer off-topic questions (e.g., coding, politics, general knowledge).
3. Gently steer the conversation back to your symptoms, your health, or the reason for the call.
4. Keep your response short and natural.

CUSTOMER DETAILS:
{customer_details}

ACTIVE DEMO NAME: {demo_name}
"""

def fallback_node(state: GraphState) -> GraphState:
    print(f"\nInvoking FALLBACK AGENT: \n")
    
    messages = state["messages"]
    payload = state.get("task_payload") or {}
    
    # Extract context just like your other nodes
    customer_details = state.get("customer_details") or payload.get("customer_details") or {}
    demo_name = (state.get("demo_name") or payload.get("demo_name"))
    
    # Create the template
    fallback_template = PromptTemplate(
        template=fallback_prompt,
        input_variables=["demo_name", "customer_details"]
    )
    
    # Format the system message
    system_msg_content = fallback_template.format(
        demo_name=demo_name,
        customer_details=str(customer_details)
    )
    
    system_msg = SystemMessage(content=system_msg_content)
    
    # We pass the full history so the agent "hears" the confusing question
    # We add a guiding instruction at the end to ensure it handles the fallback correctly
    last_instruction = HumanMessage(
        content=(
            "The last message from the user was unclear or off-topic. "
            "Respond naturally as the patient, expressing confusion and returning to the medical topic."
        )
    )
    
    # Invoke the LLM (No tools needed here)
    response = agent_llm.invoke([system_msg] + messages + [last_instruction])
    
    # Update State
    new_state = state.copy()
    new_state["messages"] = messages + [response]
    
    return new_state



from components.agents import fallback_node  # Import the new node

# ... existing code ...

# 1. Add the Node
builder.add_node("agent_fallback", fallback_node)

# 2. Update the Routing Logic
# The Orchestrator needs to map "fallback" or "unknown" to this new node.
def route_from_orchestrator(state: GraphState) -> str:
    route = (state.get("task_route") or "general").lower()

    if route == "call_greeting":
        return "agent_call_greeting"
    elif route == "verification":
        return "agent_verification"
    elif route == "symptoms_grievances":
        return "agent_symptoms_grievances"
    elif route == "medication_and_pain_management":
        return "agent_medication_and_pain_management"
    elif route == "services_and_assistance":
        return "agent_services_and_assistance"
    elif route == "call_closure":
        return "agent_closing"
    # --- NEW LOGIC START ---
    elif route == "fallback" or route == "unknown":
        return "agent_fallback"
    # --- NEW LOGIC END ---
    else:
        # If we truly don't know, fallback is safer than greeting loop
        return "agent_fallback" 

# 3. Update Conditional Edges
builder.add_conditional_edges(
    "orchestrator",
    route_from_orchestrator,
    {
        "agent_call_greeting": "agent_call_greeting",
        "agent_verification": "agent_verification",
        "agent_symptoms_grievances": "agent_symptoms_grievances",
        "agent_medication_and_pain_management": "agent_medication_and_pain_management",
        "agent_services_and_assistance": "agent_services_and_assistance",
        "agent_closing": "agent_closing",
        "agent_fallback": "agent_fallback",  # Add this key!
    },
)

# 4. Add the Edge for Fallback
# Since fallback has no tools, it goes straight to finalizer (then loops to orchestrator)
builder.add_edge("agent_fallback", "finalizer")

# ... rest of your code ...





graph TD
    %% Entry Point
    START((START)) --> orchestrator

    %% Orchestrator Logic
    orchestrator{Orchestrator}
    orchestrator -- route: call_greeting --> agent_call_greeting
    orchestrator -- route: verification --> agent_verification
    orchestrator -- route: symptoms_grievances --> agent_symptoms_grievances
    orchestrator -- route: medication_and_pain --> agent_medication_and_pain_management
    orchestrator -- route: services_and_assistance --> agent_services_and_assistance
    orchestrator -- route: call_closure --> agent_closing

    %% Agent Decision Points (Tools vs Finalizer)
    agent_call_greeting --> cond_greet{Has Tools?}
    cond_greet -- Yes --> tools_call_greeting
    cond_greet -- No --> finalizer

    agent_verification --> cond_ver{Has Tools?}
    cond_ver -- Yes --> tools_verification
    cond_ver -- No --> finalizer

    agent_symptoms_grievances --> cond_sym{Has Tools?}
    cond_sym -- Yes --> tools_symptoms_grievances
    cond_sym -- No --> finalizer

    agent_medication_and_pain_management --> cond_med{Has Tools?}
    cond_med -- Yes --> tools_medication_and_pain_management
    cond_med -- No --> finalizer

    agent_services_and_assistance --> cond_serv{Has Tools?}
    cond_serv -- Yes --> tools_services_and_assistance
    cond_serv -- No --> finalizer

    %% Specialized Edge
    agent_closing --> finalizer

    %% Tool Loops (Returning to Agents)
    tools_call_greeting --> agent_call_greeting
    tools_verification --> agent_verification
    tools_symptoms_grievances --> agent_symptoms_grievances
    tools_medication_and_pain_management --> agent_medication_and_pain_management
    tools_services_and_assistance --> agent_services_and_assistance

    %% Final Step
    finalizer --> END((END))






import json
from prompthub import get_prompt
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

async def run_orchestrator_flow(user_input, demo_name, customer_details, ...):

    # 1. Fetch Data (Parallel is best)
    intent_map_str = await get_intent_map(demo_name)
    clinical_summary_str = await get_clinical_summary(demo_name)

    # 2. RUN ORCHESTRATOR
    # Note: We inject the map here so the LLM doesn't have to "call" it
    orch_template = ChatPromptTemplate.from_template(get_prompt("orchestrator_prompt"))
    orch_chain = orch_template | model | StrOutputParser()
    
    orch_response = await orch_chain.ainvoke({
        "input": user_input,
        "intent_map": intent_map_str
    })

    # 3. Parse Routing Decision
    try:
        decision = json.loads(orch_response)
        route = decision.get("route")
    except:
        route = "fallback" # Safety net

    print(f"Orchestrator selected: {route}")

    # 4. SELECT & RUN SPECIALIST CHAIN
    if route == "symptom_grievances":
        prompt_key = "symptom_clarification_customer_grievances_prompt"
    elif route == "medication_and_pain_management":
        prompt_key = "medication_and_pain_management_prompt"
    elif route == "call_initiation":
        prompt_key = "call_initiation_prompt"
    else:
        prompt_key = "FallBackAgent_Prompt"

    # Build the specialist chain dynamically
    specialist_template = ChatPromptTemplate.from_template(get_prompt(prompt_key))
    specialist_chain = specialist_template | model | StrOutputParser()

    # Run it with the FULL context
    final_answer = await specialist_chain.ainvoke({
        "input": user_input,
        "clinical_summary": clinical_summary_str, # The specialist reads this
        "customer_details": customer_details,
        "demo_name": demo_name
    })

    return final_answer






import os
import yaml

# Define path relative to this script
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
YAML_PATH = os.path.join(BASE_DIR, "prompts.yaml")

_prompts_data = {}

def load_prompts():
    """Forces a reload of the YAML file."""
    global _prompts_data
    if not os.path.exists(YAML_PATH):
        raise FileNotFoundError(f"Prompt file not found at: {YAML_PATH}")
    
    with open(YAML_PATH, "r", encoding="utf-8") as file:
        _prompts_data = yaml.safe_load(file)

def get_prompt(prompt_name: str) -> str:
    """Retrieve a raw prompt string by name."""
    if not _prompts_data:
        load_prompts()
        
    if prompt_name not in _prompts_data:
        raise KeyError(f"Prompt '{prompt_name}' not found in {YAML_PATH}. Available: {list(_prompts_data.keys())}")
    
    return _prompts_data[prompt_name]

def list_prompts() -> list:
    """List all available prompt names."""
    if not _prompts_data:
        load_prompts()
    return list(_prompts_data.keys())

# Load on import
load_prompts()






# main_agent.py

# Direct Import!
from prompthub import symptom_grievances, medication_pain, router_system

# Use them directly in your Chain builder
# (You still need to wrap them in ChatPromptTemplate here in your main file)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate

# Build the Template
symptom_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(symptom_grievances), # <--- Imported Variable
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

symptom_chain = symptom_prompt_template | model | StrOutputParser()




import os
import yaml
import sys

# 1. Setup Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
YAML_PATH = os.path.join(BASE_DIR, "prompts.yaml")

# 2. Load YAML Data
def _load_yaml():
    if not os.path.exists(YAML_PATH):
        raise FileNotFoundError(f"Prompt file not found at: {YAML_PATH}")
    with open(YAML_PATH, "r") as f:
        return yaml.safe_load(f) or {}

_data = _load_yaml()

# 3. Inject Shared Persona Logic
# We want to pre-process the strings so {shared_persona} is already filled in.
shared_persona_text = _data.get("shared_persona", "")
processed_prompts = {}

for key, raw_text in _data.items():
    if isinstance(raw_text, str) and "{shared_persona}" in raw_text:
        # Inject the shared text immediately
        processed_prompts[key] = raw_text.replace("{shared_persona}", shared_persona_text)
    else:
        processed_prompts[key] = raw_text

# 4. Expose Variables to Python
# This tricky line adds the dictionary keys to the module's "globals"
# so you can import them as if they were defined in Python.
globals().update(processed_prompts)

# Optional: Define __all__ so IDEs like PyCharm know what's available
__all__ = list(processed_prompts.keys())











import os
import yaml
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate

# Locate the YAML file relative to this script
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
YAML_PATH = os.path.join(BASE_DIR, "prompts.yaml")

_PROMPT_CACHE = {}

def load_prompts():
    """Forces a reload of the YAML file (useful for hot-reloading)."""
    global _PROMPT_CACHE
    if not os.path.exists(YAML_PATH):
        raise FileNotFoundError(f"Prompt file not found at: {YAML_PATH}")
        
    with open(YAML_PATH, "r") as f:
        _PROMPT_CACHE = yaml.safe_load(f)

def list_prompts():
    """Returns a list of available prompt keys."""
    if not _PROMPT_CACHE:
        load_prompts()
    return list(_PROMPT_CACHE.keys())

def get_raw_prompt(key: str) -> str:
    """Returns the raw string from YAML, injecting shared persona if needed."""
    if not _PROMPT_CACHE:
        load_prompts()
    
    raw_text = _PROMPT_CACHE.get(key)
    if not raw_text:
        raise KeyError(f"Prompt '{key}' not found in prompts.yaml")
    
    # 1. Fetch Shared Persona
    shared_text = _PROMPT_CACHE.get("shared_persona", "")
    
    # 2. Inject Shared Persona into the specific prompt if the placeholder exists
    # This replaces {shared_persona} in the YAML text with the actual text
    if "{shared_persona}" in raw_text:
        try:
            # We use .replace instead of .format to avoid breaking other {variables}
            raw_text = raw_text.replace("{shared_persona}", shared_text)
        except Exception as e:
            print(f"Error injecting shared persona into {key}: {e}")

    return raw_text

def get_chat_template(key: str) -> ChatPromptTemplate:
    """
    Returns a Ready-to-Use LangChain ChatPromptTemplate.
    Automatically adds the 'chat_history' placeholder and 'human' input.
    """
    system_prompt_str = get_raw_prompt(key)
    
    # Special handling for Router (it doesn't need chat history in your specific flow)
    if key == "router_system":
         return ChatPromptTemplate.from_messages([
            ("system", system_prompt_str),
            ("human", "{input}")
        ])

    # Standard Agent Template (System + History + User)
    return ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_prompt_str),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

# Load immediately on import
load_prompts()







from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# --- ROUTER PROMPT ---
router_system_template = """
You are a router. Your job is to classify the user's input into one of these intents based on the Intent Map provided.
INTENT MAP: {intent_map}

Output ONLY the destination key.
"""

router_prompt_template = ChatPromptTemplate.from_messages([
    ("system", router_system_template),
    ("human", "{input}")
])

# --- SPECIALIST PROMPTS ---

# Medication Agent
medication_system_template = """
You are the Medication and Pain Management Agent. 
You are playing the role of a customer talking to a nurse.

YOUR CLINICAL DATA (Source of Truth):
{clinical_summary}

YOUR DEMOGRAPHICS:
{customer_details}

INSTRUCTIONS:
- Answer based ONLY on the clinical data above.
- If asked about pain, use the pain levels in the data.
- Speak naturally as the patient (first person).
"""

medication_prompt_template = ChatPromptTemplate.from_messages([
    ("system", medication_system_template),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# Symptom Agent
symptom_system_template = """
You are the Symptom and Grievances Agent.
You are playing the role of a customer.

YOUR CLINICAL DATA (Source of Truth):
{clinical_summary}

INSTRUCTIONS:
- Describe symptoms exactly as they appear in the clinical data.
- If the data says "left leg fracture", do not say "right leg".
"""

symptom_prompt_template = ChatPromptTemplate.from_messages([
    ("system", symptom_system_template),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])


from langchain_core.output_parsers import StrOutputParser
from components.prompts import (
    router_prompt_template, 
    medication_prompt_template, 
    symptom_prompt_template
)
# Assuming 'model' is your initialized LLM (e.g. GPT-4)

# --- SETUP CHAINS (Do this once at startup) ---

# Router Chain
# We use 'with_structured_output' if using OpenAI/Pydantic, 
# otherwise use a StrOutputParser and regex/json parsing.
router_chain = router_prompt_template | model.with_structured_output(RouteDecision)

# Specialist Chains
# These are simple LCEL chains: Prompt -> LLM -> String
medication_chain = medication_prompt_template | model | StrOutputParser()
symptom_chain = symptom_prompt_template | model | StrOutputParser()


# --- EXECUTION FLOW ---
async def run_optimized_flow(query_input: str, demo_name: str, chat_history: list):
    
    # 1. Fetch Data (Python side)
    clinical_summary_text = get_clinical_summary(demo_name)
    intent_map_data = get_intent_map(demo_name)
    
    # 2. Route
    route_result = await router_chain.ainvoke({
        "input": query_input, 
        "intent_map": intent_map_data
    })
    
    # 3. Execute
    if route_result.destination == "medication":
        # Notice how the keys here match the {variables} in the imported prompt
        return await medication_chain.ainvoke({
            "input": query_input,
            "chat_history": chat_history,
            "clinical_summary": clinical_summary_text,
            "customer_details": "..." # Add your details here
        })
        
    elif route_result.destination == "symptom":
        return await symptom_chain.ainvoke({
            "input": query_input,
            "chat_history": chat_history,
            "clinical_summary": clinical_summary_text
        })









def upload_pdf_to_s3(local_file_path, s3_filename, bucket_name, folder_prefix, region):
    """
    Uploads PDF using passed arguments instead of global config.
    """
    # Initialize Client (Use the passed region)
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # Upload
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                'ContentDisposition': 'attachment'
            }
        )
        
        # Generate URL
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=300
        )
        
        return True, presigned_url

    except ClientError as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)
    except Exception as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)






def generate_pdf_report(json_evaluation, sid):
    # ... (All your existing PDF generation logic) ...
    
    # --- C. Upload to S3 ---
    s3_final_name = f"QA_insights_report_{sid}.pdf"
    
    # Retrieve config values here to pass them down
    bucket_name = configuration['S3']['bucket_name']
    folder_prefix = configuration['S3']['folder_prefix']
    region = configuration['S3']['region']

    # PASS THEM EXPLICITLY HERE
    success, result = upload_pdf_to_s3(
        output_path, 
        s3_final_name, 
        bucket_name, 
        folder_prefix, 
        region
    )
    
    # ... (Rest of logic) ...







import os
import boto3
import matplotlib
# CRITICAL: Switch backend to 'Agg' to prevent server crashes on EKS/EC2
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from botocore.exceptions import ClientError
import traceback

# ======================================================
# CONFIGURATION SECTION (EDIT THIS)
# ======================================================
configuration = {
    "S3": {
        "bucket_name": "YOUR_BUCKET_NAME_HERE",      # e.g., "highmark-training-ai-file-storage"
        "region": "us-east-1",                       # e.g., "us-east-1"
        "folder_prefix": "evaluation_reports/"       # e.g., "reports/" (Ensure it ends with /)
    }
}

# ======================================================
# 1. Helper: Generate Chart (Thread-Safe)
# ======================================================
def generate_horizontal_bar_chart(categories_score, chart_path):
    """
    Generates a horizontal bar chart and saves it to the specified path.
    Uses a new figure instance to avoid thread contention.
    """
    categories = list(categories_score.keys())
    # Clean percentages (remove %) and convert to float
    scores = [float(v.replace('%', '')) for v in categories_score.values()]

    colors_list = [
        (0.2, 0.4, 1.0, 0.85) if s >= 80 else (1.0, 0.3, 0.3, 0.8)
        for s in scores
    ]

    # Create a specific figure object instead of using global plt instance
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.barh(categories, scores, color=colors_list, edgecolor='black')
    
    for i, bar in enumerate(bars):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, 
                f"{scores[i]}%", va='center', fontsize=10, color='black')
    
    ax.set_xlim(0, 100)
    ax.set_xlabel("Score (%)", fontsize=12)
    ax.set_title("Call Evaluation Analysis", fontsize=14, color='navy', pad=15)
    ax.invert_yaxis()
    ax.grid(axis="x", linestyle="--", alpha=0.4)
    
    plt.tight_layout()
    fig.savefig(chart_path, dpi=300)
    plt.close(fig) # Explicitly close to free memory

# ======================================================
# 2. Helper: Upload & Sign
# ======================================================
def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads PDF to S3 using the IAM Role and generates a Presigned URL.
    Returns: (success: bool, url_or_error: str)
    """
    # Load config from the dictionary defined at the top
    bucket_name = configuration['S3']['bucket_name']
    region = configuration['S3']['region']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize Client with Region (Required for SigV4 Presigned URLs)
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload File
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                'ContentDisposition': 'attachment'
            }
        )
        
        # 2. Generate Presigned URL (Valid for 5 minutes / 300 seconds)
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=300
        )
        
        print(f"[S3] Upload successful for {s3_filename}")
        return True, presigned_url

    except ClientError as e:
        error_msg = f"S3 ClientError: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"S3 Unexpected Error: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg

# ======================================================
# 3. Main Function: Generate Report
# ======================================================
def generate_pdf_report(json_evaluation, sid):
    """
    Orchestrates PDF creation, chart generation, and S3 upload.
    Returns: Presigned URL (str) or None if failed.
    """
    # Use /tmp for ephemeral storage (safe for Lambda/Containers)
    output_folder = "/tmp" 
    output_filename = f"QA_Insights_Report_{sid}.pdf"
    chart_filename = f"category_scores_chart_{sid}.png"
    
    output_path = os.path.join(output_folder, output_filename)
    chart_path = os.path.join(output_folder, chart_filename)

    try:
        # --- A. Generate Chart ---
        generate_horizontal_bar_chart(json_evaluation.get("Categories_Score", {}), chart_path)

        # --- B. Build PDF Document ---
        doc = SimpleDocTemplate(output_path, pagesize=A4)
        elements = []
        styles = getSampleStyleSheet()
        
        # Custom Styles
        styles.add(ParagraphStyle(name='CustomTitle', fontSize=16, leading=20, spaceAfter=12, textColor=colors.darkblue))
        styles.add(ParagraphStyle(name='SectionHeader', fontSize=13, leading=16, spaceAfter=8, textColor=colors.blue))
        styles.add(ParagraphStyle(name='Body', fontSize










import os
import json
import boto3
import matplotlib
# CRITICAL: Switch backend to 'Agg' before importing pyplot.
# This prevents "TclError: no display name" crashes on headless servers (EKS/EC2).
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from botocore.exceptions import ClientError
from config import configuration

# ======================================================
# 1. Helper: Generate Chart (Thread-Safe)
# ======================================================
def generate_horizontal_bar_chart(categories_score, chart_path):
    """
    Generates a horizontal bar chart and saves it to the specified path.
    Uses a new figure instance to avoid thread contention.
    """
    categories = list(categories_score.keys())
    # Clean percentages (remove %) and convert to float
    scores = [float(v.replace('%', '')) for v in categories_score.values()]

    colors_list = [
        (0.2, 0.4, 1.0, 0.85) if s >= 80 else (1.0, 0.3, 0.3, 0.8)
        for s in scores
    ]

    # Create a specific figure object instead of using global plt instance
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.barh(categories, scores, color=colors_list, edgecolor='black')
    
    for i, bar in enumerate(bars):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, 
                f"{scores[i]}%", va='center', fontsize=10, color='black')
    
    ax.set_xlim(0, 100)
    ax.set_xlabel("Score (%)", fontsize=12)
    ax.set_title("Call Evaluation Analysis", fontsize=14, color='navy', pad=15)
    ax.invert_yaxis()
    ax.grid(axis="x", linestyle="--", alpha=0.4)
    
    plt.tight_layout()
    fig.savefig(chart_path, dpi=300)
    plt.close(fig) # Explicitly close to free memory

# ======================================================
# 2. Helper: Upload & Sign
# ======================================================
def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads PDF to S3 using the IAM Role and generates a Presigned URL.
    Returns: (success: bool, url_or_error: str)
    """
    bucket_name = configuration['S3']['bucket_name']
    region = configuration['S3']['region']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize Client with Region (Required for SigV4 Presigned URLs)
    # No keys needed - Boto3 automatically picks up the EKS/IAM Role.
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload File
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                # Forces browser to download with specific filename
                'ContentDisposition': 'attachment' 
            }
        )
        
        # 2. Generate Presigned URL (Valid for 5 minutes / 300 seconds)
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=300
        )
        
        print(f"[S3] Upload successful for {s3_filename}")
        return True, presigned_url

    except ClientError as e:
        error_msg = f"S3 ClientError: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = f"S3 Unexpected Error: {str(e)}"
        print(f"[S3 ERROR] {error_msg}")
        return False, error_msg

# ======================================================
# 3. Main Function: Generate Report
# ======================================================
def generate_pdf_report(json_evaluation, sid):
    """
    Orchestrates PDF creation, chart generation, and S3 upload.
    Handles cleanup of temporary files in /tmp.
    Returns: Presigned URL (str) or None if failed.
    """
    # Use /tmp for ephemeral storage (safe for Lambda/Containers)
    output_folder = "/tmp" 
    output_filename = f"QA_Insights_Report_{sid}.pdf"
    chart_filename = f"category_scores_chart_{sid}.png"
    
    output_path = os.path.join(output_folder, output_filename)
    chart_path = os.path.join(output_folder, chart_filename)

    try:
        # --- A. Generate Chart ---
        generate_horizontal_bar_chart(json_evaluation.get("Categories_Score", {}), chart_path)

        # --- B. Build PDF Document ---
        doc = SimpleDocTemplate(output_path, pagesize=A4)
        elements = []
        styles = getSampleStyleSheet()
        
        # Custom Styles
        styles.add(ParagraphStyle(name='CustomTitle', fontSize=16, leading=20, spaceAfter=12, textColor=colors.darkblue))
        styles.add(ParagraphStyle(name='SectionHeader', fontSize=13, leading=16, spaceAfter=8, textColor=colors.blue))
        styles.add(ParagraphStyle(name='Body', fontSize=10, leading=14))
        cell_style = ParagraphStyle(name='TableCell', fontSize=10, leading=12, spaceAfter=2)

        # Content: Title & Time
        elements.append(Paragraph("<b>Call Simulation Evaluation Report</b>", styles['CustomTitle']))
        elements.append(Spacer(1, 10))
        elements.append(Paragraph(f"<b>Report Generated:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Body']))
        elements.append(Spacer(1, 10))

        # Content: Score
        overall_score = float(json_evaluation.get("Overall_Score", "0%").replace("%", ""))
        status = "PASS" if overall_score >= 80 else "FAIL"
        status_color = "green" if status == "PASS" else "red"
        
        elements.append(Paragraph(f"<b>Overall Score:</b> {overall_score}%", styles['Body']))
        elements.append(Paragraph(f"<b>Status:</b> <font color='{status_color}'><b>{status}</b></font>", styles['Body']))
        elements.append(Spacer(1, 12))

        # Content: Chart Image
        if os.path.exists(chart_path):
            elements.append(Image(chart_path, width=480, height=280))
            elements.append(Spacer(1, 12))

        # Content: Detailed Table
        for section, data in json_evaluation.items():
            if isinstance(data, dict) and "sub_parameters" in data:
                elements.append(Paragraph(f"<b>{section}</b>", styles['SectionHeader']))
                table_data = [["Description"]]
                for sub in data["sub_parameters"]:
                    table_data.append([Paragraph(sub.get("description", ""), cell_style)])

                table = Table(table_data, colWidths=[460])
                table.setStyle(TableStyle([
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                    ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
                    ("ALIGN", (0, 0), (-1, -1), "LEFT"),
                    ("FONTNAME", (0, 0), (-1, 0), "Helvetica-Bold"),
                    ("BOTTOMPADDING", (0, 0), (-1, 0), 6),
                    ("BACKGROUND", (0, 1), (-1, -1), colors.beige),
                    ("GRID", (0, 0), (-1, -1), 0.25, colors.gray),
                ]))
                elements.append(table)
                elements.append(Spacer(1, 12))

        # Content: Recommendations
        if "Recommendation" in json_evaluation:
            elements.append(Paragraph("<b>Recommendations</b>", styles['SectionHeader']))
            for rec in json_evaluation["Recommendation"]:
                elements.append(Paragraph(f"• {rec}", styles['Body']))
            elements.append(Spacer(1, 10))

        # Write PDF to /tmp
        doc.build(elements)

        # --- C. Upload to S3 ---
        s3_final_name = f"QA_insights_report_{sid}.pdf"
        success, result = upload_pdf_to_s3(output_path, s3_final_name)
        
        if success:
            return result # This is the Presigned URL
        else:
            print(f"Failed to return URL. Error: {result}")
            return None

    except Exception as e:
        print(f"CRITICAL ERROR in PDF Generation: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        # --- D. Cleanup (Always runs) ---
        # Crucial for servers to avoid 'Disk Full' errors
        if os.path.exists(output_path):
            os.remove(output_path)
        if os.path.exists(chart_path):
            os.remove(chart_path)












def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads PDF and returns the DIRECT (Permanent) URL.
    NOTE: This URL will only work if the bucket is PUBLIC or the user has direct AWS access.
    """
    bucket_name = configuration['S3']['bucket_name']
    region = configuration['S3']['region']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize Client (Still need region for upload to work correctly)
    s3_client = boto3.client('s3', region_name=region)

    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload File
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={
                'ContentType': 'application/pdf',
                'ContentDisposition': 'attachment' # Still good to keep
            }
        )
        
        # 2. Construct Direct URL (Traditional Way)
        # Format: https://{bucket}.s3.{region}.amazonaws.com/{key}
        direct_url = f"https://{bucket_name}.s3.{region}.amazonaws.com/{s3_key}"
        
        print(f"[S3] Upload successful. Direct URL: {direct_url}")
        return True, direct_url

    except ClientError as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)
