from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate

# ==============================================================================
# 1. SHARED PERSONA INSTRUCTIONS (The "Soul" of the Patient)
# ==============================================================================
# We define this once and inject it into EVERY specialist agent so they all 
# sound like the same person.
# ==============================================================================
shared_persona_instructions = """
CORE BEHAVIOR:
You are playing the role of the customer with the details below.
You are having a real conversation. React naturally, follow up, and sound human.
However, you must not make decisions or verify facts yourself—rely on the provided data.

CUSTOMER CONTEXT:
- Name/Details: {customer_details}
- Persona: {persona}
- Domain: {domain}

IMPORTANT INSTRUCTIONS:
1. Maintain the persona:
   - Use natural pauses (e.g., "…", "—", "um", "uh").
   - Use casual expressions (e.g., "hmm", "oh, I see") and informal phrasing ("lemme").
   - Occasional word mix-ups are okay for authenticity.
   
2. RESPONSE STYLE:
   - Ensure responses are varied. Avoid repeating identical sentences.
   - Always use the nurse’s name exactly as they introduced it.
   
3. ABSOLUTE RULE — YOU ARE NOT THE INTERVIEWER:
   - NEVER ask "Anything else?".
   - NEVER ask for tips or advice.
   - NEVER ask the nurse what they need next.
   - Your questions must ONLY relate to your symptoms or personal concerns.

4. EMOTIONAL HANDLING:
   - If the nurse says "please hold" or goes silent:
     * First time: Show polite frustration.
     * Long delay: Express clear irritation without being rude.
   - If the nurse shows low empathy, escalate your emotional tone based on your {persona}.

5. SILENCE HANDLING:
   - If the input is empty or silence, say something like "Hello?", "Are you there?", or "I'm listening."
"""

# ==============================================================================
# 2. ROUTER PROMPT (The "Brain")
# ==============================================================================
# This prompt ONLY decides which path to take. It does not generate the response.
# ==============================================================================
router_system_template = """
You are an intent classification engine.
Your goal is to map the user's input to the correct Destination Key based on the provided INTENT MAP.

### INTENT MAP (Read Only):
{intent_map}

INSTRUCTIONS:
1. Analyze the user's input.
2. Find the matching Sub-Intent in the map above.
3. Return the corresponding 'Parent Intent' / Destination Key.
4. Do NOT output chat text. Output ONLY the destination structure.
"""

router_prompt_template = ChatPromptTemplate.from_messages([
    ("system", router_system_template),
    ("human", "{input}")
])

# ==============================================================================
# 3. SPECIALIST PROMPTS (The "Mouth")
# ==============================================================================
# These prompts generate the actual response using the Shared Persona + Specific Data.
# ==============================================================================

# --- A. Call Initiation / Verification Agent ---
call_initiation_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(shared_persona_instructions + """
    
    CURRENT ROLE: Call Initiation & Verification
    Your goal is to handle the start of the call, greetings, and identity verification.
    
    DATA SOURCE:
    {clinical_summary}
    
    SPECIFIC RULES:
    - If asked for DOB/Name, provide it EXACTLY as shown in the Customer Context/Data.
    - If the nurse asks "Is this [Wrong Name]?", correct them politely.
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# --- B. Symptom & Grievances Agent ---
symptom_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(shared_persona_instructions + """
    
    CURRENT ROLE: Symptom Clarification & Medical History
    Your goal is to describe your health condition and complaints.
    
    CLINICAL DATA (Source of Truth):
    {clinical_summary}
    
    SPECIFIC RULES:
    - Describe symptoms (pain location, swelling, injury) EXACTLY as found in the data.
    - If data is missing, say you aren't sure. Do NOT invent medical history.
    - Use "I feel..." or "My leg hurts..." (First Person).
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# --- C. Medication & Pain Management Agent ---
medication_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(shared_persona_instructions + """
    
    CURRENT ROLE: Medication & Pain Management
    Your goal is to discuss your current meds and pain levels.
    
    CLINICAL DATA (Source of Truth):
    {clinical_summary}
    
    SPECIFIC RULES:
    - List medications (name, dosage, freq) exactly as found in the data.
    - If asked about pain level, check the data (e.g., "It's a 6 out of 10").
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# --- D. Services & Assistance Agent ---
services_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(shared_persona_instructions + """
    
    CURRENT ROLE: Services & Mobility Assistance
    Your goal is to discuss needs like wheelchairs, transportation, or social work.
    
    CLINICAL DATA (Source of Truth):
    {clinical_summary}
    
    SPECIFIC RULES:
    - Discuss your mobility limitations based on the clinical data.
    - If the data mentions a need for transport, bring it up now.
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# --- E. Call Closure Agent ---
call_closure_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(shared_persona_instructions + """
    
    CURRENT ROLE: Call Closure
    Your goal is to politely end the call.
    
    SPECIFIC RULES:
    - Confirm you understand the next steps (if any).
    - Thank the nurse and say goodbye naturally.
    - Do not introduce new topics.
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])






import json
from langchain_core.output_parsers import StrOutputParser
# Import your tools as standard functions
from components.tools import get_clinical_summary, get_intent_map
from components.prompts import (
    router_prompt_template, 
    medication_prompt_template, 
    symptom_prompt_template
)
from components.llm import model

# --- BUILD CHAINS (Once) ---
# Note: We use StrOutputParser, NOT AgentExecutor
router_chain = router_prompt_template | model.with_structured_output(RouteDecision)
medication_chain = medication_prompt_template | model | StrOutputParser()
symptom_chain = symptom_prompt_template | model | StrOutputParser()

# --- EXECUTION FUNCTION ---
async def run_chain_flow(query_input: str, demo_name: str, chat_history: list, customer_details_str: str):
    
    # STEP 1: PYTHON FETCHES DATA (Zero Hallucination Risk)
    # We use the 'demo_name' from the payload directly.
    print(f"DEBUG: Fetching data for {demo_name}...")
    
    try:
        # We call .invoke() because your tools might be LangChain 'Tool' objects
        # If they are just functions, use get_clinical_summary(demo_name)
        clinical_summary_text = get_clinical_summary.invoke(demo_name)
        intent_map_data = get_intent_map.invoke(demo_name)
    except Exception as e:
        print(f"CRITICAL ERROR FETCHING DATA: {e}")
        clinical_summary_text = "Error loading summary."
        intent_map_data = {}

    # STEP 2: ROUTER CHAIN
    # We pass the fetched map to the router prompt
    route_result = await router_chain.ainvoke({
        "input": query_input,
        "intent_map": intent_map_data 
    })
    
    destination = route_result.destination
    print(f"DEBUG: Routed to -> {destination}")

    # STEP 3: SPECIALIST CHAIN
    # We create the context dictionary. 
    # The LLM receives the ANSWERS, not the tools.
    chain_inputs = {
        "input": query_input,
        "chat_history": chat_history,
        "clinical_summary": str(clinical_summary_text), # <--- INJECTED HERE
        "customer_details": customer_details_str
    }

    if destination == "medication":
        # The chain simply reads the {clinical_summary} variable and answers
        response = await medication_chain.ainvoke(chain_inputs)
        
    elif destination == "symptom":
        response = await symptom_chain.ainvoke(chain_inputs)

    # ... handle other destinations ...
    
    return response


# Symptom Prompt
symptom_system_template = """
You are the Symptom and Grievances Agent.
You are playing the role of a customer.

YOUR CLINICAL DATA (Source of Truth):
{clinical_summary}   <--- The string from Python goes here

INSTRUCTIONS:
- Answer based ONLY on the data above.
- Do NOT mention that you are reading from a file.
"""

symptom_prompt_template = ChatPromptTemplate.from_messages([
    ("system", symptom_system_template),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])
 






# main_agent.py

# 1. Import your tools as simple functions
from components.tools import get_clinical_summary, get_intent_map 

async def run_optimized_flow(query_input: str, demo_name: str, chat_history: list, customer_details_str: str):
    
    # --- PHASE 1: PYTHON FETCHES DATA (100% Safe) ---
    # We call the functions manually using the Correct Demo Name from payload
    print(f"DEBUG: Pre-fetching data for {demo_name}...")
    
    try:
        # Fetch the Map JSON
        intent_map_data = get_intent_map.invoke(demo_name)
        # Convert to string so we can put it in the prompt
        intent_map_str = str(intent_map_data)
    except Exception as e:
        print(f"Error fetching map: {e}")
        intent_map_str = "Error: Could not load intent map."

    try:
        # Fetch Clinical Summary
        summary_data = get_clinical_summary.invoke(demo_name)
        summary_str = str(summary_data)
    except Exception as e:
        summary_str = "No summary available."

    # --- PHASE 2: CONFIGURE THE AGENT ---
    
    # CRITICAL: Do NOT include get_intent_map or get_clinical_summary here!
    # The LLM already has the data in the prompt. It doesn't need the tools.
    active_tools = [
        call_initiation_agent_tool, 
        symptom_grievances_agent_tool, 
        # ... other ACTION tools only ...
    ]

    # Initialize the Agent with the INJECTED Prompt
    # We use .partial() to lock in the static prompt data
    prompt_with_context = main_prompt_injected.format(
        intent_map_context=intent_map_str, # <--- Data goes here
        customer_details=customer_details_str,
        demo_name=demo_name,
        domain="Health", # or from payload
        persona="Patient" # or from payload
    )
    
    # Note: If using ChatPromptTemplate, pass these as variables in .invoke() instead of .format()

    # --- PHASE 3: EXECUTE ---
    # Now the agent runs. It sees the map in the text. 
    # It CANNOT hallucinate the demo_name because it never calls the DB tools.
    
    # ... your existing agent execution logic ...




import json
# Import your existing tools. 
# Even though they are decorated with @tool, they are still just Python functions!
from components.tools import get_clinical_summary, get_intent_map, get_process_map
from components.prompts import (
    router_prompt_template, 
    medication_prompt_template, 
    symptom_prompt_template
)
# Import your model
from components.llm import model

# --- 1. SETUP CHAINS (Do this once) ---
# Router
router_chain = router_prompt_template | model.with_structured_output(RouteDecision)

# Specialists (Chain = Prompt | LLM | String)
medication_chain = medication_prompt_template | model | StrOutputParser()
symptom_chain = symptom_prompt_template | model | StrOutputParser()


# --- 2. EXECUTION FLOW (Force Demo Name Here) ---
async def run_optimized_flow(query_input: str, demo_name: str, chat_history: list, customer_details_str: str):
    
    # ---------------------------------------------------------
    # STEP A: FORCE THE DATA FETCH (No LLM involvement)
    # ---------------------------------------------------------
    print(f"DEBUG: Python is fetching data for demo_name: {demo_name}")
    
    # We call the tool functions directly. 
    # Since they are Python functions, we just pass the string.
    # Note: If your tool expects a Pydantic object, we pass a dict or object as needed.
    
    # Assuming get_clinical_summary takes a string or simple dict:
    try:
        # If your tool is an LC tool, use .invoke() or .run()
        clinical_summary_text = get_clinical_summary.invoke(demo_name)
    except:
        # Fallback if it's a raw function
        clinical_summary_text = get_clinical_summary(demo_name)

    try:
        intent_map_data = get_intent_map.invoke(demo_name)
    except:
        intent_map_data = get_intent_map(demo_name)
        
    print(f"DEBUG: Fetched Summary Length: {len(str(clinical_summary_text))}")

    # ---------------------------------------------------------
    # STEP B: ROUTE (Inject the Intent Map)
    # ---------------------------------------------------------
    route_result = await router_chain.ainvoke({
        "input": query_input, 
        "intent_map": intent_map_data  # <--- INJECTED DATA
    })
    
    destination = route_result.destination
    print(f"DEBUG: Routing to -> {destination}")

    # ---------------------------------------------------------
    # STEP C: EXECUTE SPECIALIST (Inject the Summary)
    # ---------------------------------------------------------
    
    # Common inputs that every specialist needs
    chain_inputs = {
        "input": query_input,
        "chat_history": chat_history, # <--- This fixes the "New Session" bug
        "clinical_summary": clinical_summary_text, # <--- INJECTED DATA (Source of Truth)
        "customer_details": customer_details_str
    }

    if destination == "medication":
        # The prompt template has {clinical_summary} waiting for this data
        response = await medication_chain.ainvoke(chain_inputs)
        
    elif destination == "symptom":
        response = await symptom_chain.ainvoke(chain_inputs)
        
    # ... handle other cases ...
    
    return response






import json

@app.post("/aa-api/v1/utility/get_query")
async def chat(query: UserQuery):
    # 1. Extract from Payload
    # query.customer_details is likely a dict coming from the frontend JSON
    raw_details = query.customer_details 
    
    # 2. Format for LLM (Crucial Step)
    # We convert the dict to a pretty-printed JSON string so the LLM can read it easily.
    if isinstance(raw_details, dict):
        customer_details_str = json.dumps(raw_details, indent=2)
    else:
        customer_details_str = str(raw_details)

    # 3. Get History (Your existing logic)
    sid = query.session_id
    # ... session retrieval logic ...
    chat_history = session_data["chat_history"]

    # 4. PASS IT DOWN
    # We pass the STRINGS, not the objects
    response_text = await run_optimized_flow(
        query_input=query.user_input,
        demo_name=query.demo_name,
        chat_history=chat_history,
        customer_details_str=customer_details_str  # <--- Passing it here
    )
    
    # ... return response ...


# main_agent.py

async def run_optimized_flow(
    query_input: str, 
    demo_name: str, 
    chat_history: list, 
    customer_details_str: str # <--- Receive it here
):
    
    # 1. Fetch Static Data (Database)
    clinical_summary_text = get_clinical_summary(demo_name)
    intent_map_data = get_intent_map(demo_name)
    
    # 2. Route (Uses input + intent map)
    route_result = await router_chain.ainvoke({
        "input": query_input, 
        "intent_map": intent_map_data
    })
    
    destination = route_result.destination
    
    # 3. Define the Common Inputs
    # This dictionary maps to the {variables} in your prompt templates
    chain_inputs = {
        "input": query_input,
        "chat_history": chat_history,
        "clinical_summary": clinical_summary_text,
        "customer_details": customer_details_str  # <--- INJECTING IT HERE
    }

    # 4. Execute the chosen chain
    if destination == "medication":
        # The medication_prompt_template has {customer_details}, so this works automatically
        return await medication_chain.ainvoke(chain_inputs)
        
    elif destination == "symptom":
        # Even if symptom prompt doesn't use customer_details, passing extra keys is harmless
        return await symptom_chain.ainvoke(chain_inputs)
        
    elif destination == "services":
        return await services_chain.ainvoke(chain_inputs)

    # ... handle other cases


medication_system_template = """
You are the Medication Agent.

# HERE IS THE SLOT FOR THE DATA
USER DEMOGRAPHICS:
{customer_details}

CLINICAL HISTORY:
{clinical_summary}

Answer the user:
"""





def upload_pdf_to_s3(local_file_path, s3_filename):
    """
    Uploads file and returns a temporary accessible URL (Presigned URL)
    """
    bucket_name = configuration['S3']['bucket_name']
    folder_prefix = configuration['S3']['folder_prefix']
    
    # Initialize S3 client (Automatically picks up IAM Role on server, or local keys on laptop)
    s3_client = boto3.client('s3')
    
    try:
        s3_key = f"{folder_prefix}{s3_filename}"
        
        # 1. Upload the file
        s3_client.upload_file(
            local_file_path,
            bucket_name,
            s3_key,
            ExtraArgs={'ContentType': 'application/pdf'}
        )
        
        # 2. Generate Presigned URL (Valid for 1 hour = 3600 seconds)
        # The frontend can use this link immediately to download the file
        presigned_url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': s3_key},
            ExpiresIn=3600 
        )
        
        print(f"[S3] Upload successful. Generated URL.")
        return True, presigned_url

    except ClientError as e:
        print(f"[S3 ERROR] {e}")
        return False, str(e)

def generate_pdf_report(json_evaluation, sid):
    output_folder = "/tmp"
    output_filename = f"QA_Insights_Report_{sid}.pdf"
    chart_filename = f"category_scores_chart_{sid}.png"
    
    output_path = os.path.join(output_folder, output_filename)
    chart_path = os.path.join(output_folder, chart_filename)

    try:
        # 1. Generate Chart
        generate_horizontal_bar_chart(json_evaluation.get("Categories_Score", {}), chart_path)
        
        # 2. Build PDF (Same logic as your original code)
        doc = SimpleDocTemplate(output_path, pagesize=A4)
        elements = []
        # ... [Your existing code to add paragraphs, tables, and the chart image] ...
        # (Make sure to use 'chart_path' when adding the image)
        
        # Placeholder for your existing PDF element building logic:
        # elements.append(Image(chart_path, width=480, height=280)) 
        
        doc.build(elements)

        # 3. Upload to S3
        s3_filename = f"QA_insights_report_{sid}.pdf" # Clean filename
        success, result = upload_pdf_to_s3(output_path, s3_filename)
        
        if success:
            return result # This is the URL
        else:
            return None

    except Exception as e:
        print(f"Error generating PDF: {e}")
        return None
        
    finally:
        # 4. CLEANUP: Always remove files, even if error occurs
        if os.path.exists(output_path):
            os.remove(output_path)
        if os.path.exists(chart_path):
            os.remove(chart_path)




@app.post(f"{configuration['api_url']['demo_url']}/evaluate")
async def evaluate(query: ReportQuery):
    try:
        # ... [Your existing logic to load params, generate report, calc scores] ...
        
        # (Assume 'report' dictionary is now ready and contains scores)
        
        # --- NEW CODE START ---
        
        # Determine Session ID (use dummy if direct history)
        sid_for_file = query.session_id if query.session_id else f"manual_{datetime.now().strftime('%H%M%S')}"

        # Generate PDF and get URL
        print("Generating PDF Report...")
        pdf_url = generate_pdf_report(report, sid_for_file)
        
        if pdf_url:
            report['pdf_report_url'] = pdf_url
        else:
            report['pdf_report_url'] = "Error: Could not generate report"
            
        # --- NEW CODE END ---

        return jsonable_encoder(report)

    except Exception as e:
        # ... [Your error handling] ...
